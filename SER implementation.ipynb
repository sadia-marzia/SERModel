{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **See the speaker folder**"
      ],
      "metadata": {
        "id": "znrvAP73IBrU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "va3uwKk9J7D7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_path = '/content/drive/MyDrive/ears data'\n",
        "print(\"Contents of ears data folder:\")\n",
        "print(os.listdir(base_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resampled to 16KHz and fixed length to 15 second**"
      ],
      "metadata": {
        "id": "T8ZTvURgNImM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "# --- Parameters ---\n",
        "root_dir = \"/content/drive/MyDrive/ears data\"  # Original dataset folder containing p001, p002, ...\n",
        "output_dir = \"/content/drive/MyDrive/ears_data_15s\"  # Folder to save processed audio\n",
        "target_sr = 16000\n",
        "target_duration = 15  # seconds\n",
        "target_length = target_sr * target_duration  # 15*16000 = 240000 samples\n",
        "\n",
        "# Create output folder if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# --- Process each speaker folder ---\n",
        "for speaker in os.listdir(root_dir):\n",
        "    speaker_path = os.path.join(root_dir, speaker)\n",
        "    if not os.path.isdir(speaker_path):\n",
        "        continue\n",
        "\n",
        "    # Create corresponding output folder for speaker\n",
        "    out_speaker_path = os.path.join(output_dir, speaker)\n",
        "    os.makedirs(out_speaker_path, exist_ok=True)\n",
        "\n",
        "    # Process each audio file\n",
        "    for file_name in os.listdir(speaker_path):\n",
        "        if not file_name.endswith('.wav'):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(speaker_path, file_name)\n",
        "\n",
        "        # Load audio and resample to 16kHz\n",
        "        audio, sr = librosa.load(file_path, sr=target_sr)\n",
        "\n",
        "        # Pad or truncate to 15 seconds\n",
        "        if len(audio) < target_length:\n",
        "            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
        "        else:\n",
        "            audio = audio[:target_length]\n",
        "\n",
        "        # Save the processed audio\n",
        "        out_file_path = os.path.join(out_speaker_path, file_name)\n",
        "        sf.write(out_file_path, audio, target_sr)\n",
        "\n",
        "    print(f\"Processed all files for speaker {speaker}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FWKLvWcTNHPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MFCC extraction(.npy file)**"
      ],
      "metadata": {
        "id": "U98Pk6wPIHnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Paths ---\n",
        "audio_dir = \"/content/drive/MyDrive/ears_data_15s\"\n",
        "output_csv = \"/content/drive/MyDrive/ears_data/features_mfcc_segments.csv\"\n",
        "output_npy_dir = \"/content/drive/MyDrive/ears_data/mfcc_segments_npy\"\n",
        "\n",
        "# --- Parameters ---\n",
        "SAMPLE_RATE = 16000\n",
        "PRE_EMPHASIS = 0.97\n",
        "N_MFCC = 13\n",
        "FRAME_SIZE = 469     # frames per segment\n",
        "FRAME_OVERLAP = 128  # overlapping frames\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_npy_dir, exist_ok=True)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def pre_emphasis(signal, alpha=0.97):\n",
        "    \"\"\"Apply pre-emphasis to boost high frequencies.\"\"\"\n",
        "    return np.append(signal[0], signal[1:] - alpha * signal[:-1])\n",
        "\n",
        "def segment_mfcc(mfcc, frame_size, overlap):\n",
        "    \"\"\"Segment MFCC matrix into overlapping windows.\"\"\"\n",
        "    step = frame_size - overlap\n",
        "    segments = []\n",
        "    for start in range(0, mfcc.shape[1] - frame_size + 1, step):\n",
        "        seg = mfcc[:, start:start + frame_size]\n",
        "        segments.append(seg)\n",
        "    return np.array(segments)\n",
        "\n",
        "# --- Extraction ---\n",
        "metadata = []\n",
        "\n",
        "for speaker in sorted(os.listdir(audio_dir)):\n",
        "    speaker_path = os.path.join(audio_dir, speaker)\n",
        "    if not os.path.isdir(speaker_path):\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nðŸ”¹ Extracting MFCC segments for speaker: {speaker}\")\n",
        "    for file_name in tqdm(os.listdir(speaker_path)):\n",
        "        if not file_name.endswith(\".wav\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(speaker_path, file_name)\n",
        "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "        y = pre_emphasis(y, alpha=PRE_EMPHASIS)\n",
        "\n",
        "        # Compute MFCCs (13 coefficients)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
        "\n",
        "        # Segment into overlapping windows\n",
        "        segments = segment_mfcc(mfcc, FRAME_SIZE, FRAME_OVERLAP)\n",
        "\n",
        "        # Save each segment as a .npy file\n",
        "        for seg_idx, seg in enumerate(segments):\n",
        "            seg_filename = f\"{speaker}_{os.path.splitext(file_name)[0]}_seg{seg_idx}.npy\"\n",
        "            seg_path = os.path.join(output_npy_dir, seg_filename)\n",
        "            np.save(seg_path, seg)\n",
        "\n",
        "            metadata.append({\n",
        "                \"speaker\": speaker,\n",
        "                \"file\": file_name,\n",
        "                \"segment_index\": seg_idx,\n",
        "                \"npy_path\": seg_path\n",
        "            })\n",
        "\n",
        "# --- Save metadata ---\n",
        "df = pd.DataFrame(metadata)\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"\\nâœ… MFCC segmentation complete!\")\n",
        "print(f\"Saved segment metadata to: {output_csv}\")\n",
        "print(f\"Saved .npy MFCC segments to: {output_npy_dir}\")\n"
      ],
      "metadata": {
        "id": "5pysJq-NIGUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mel Spectogram & Chroma features extraction**"
      ],
      "metadata": {
        "id": "p9YwFiuBIoAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Set the speaker folder you want to process ===\n",
        "SPEAKER_ID = \"p098\"  # ðŸ”¹ change this for each run (e.g., p002, p003, ...)\n",
        "\n",
        "# === Paths ===\n",
        "AUDIO_DIR = f\"/content/drive/MyDrive/ears_data_15s/{SPEAKER_ID}\"\n",
        "OUTPUT_MEL_DIR = f\"/content/drive/MyDrive/ears_data/features_mel_spectrograms/{SPEAKER_ID}\"\n",
        "OUTPUT_CHROMA_DIR = f\"/content/drive/MyDrive/ears_data/features_chroma/{SPEAKER_ID}\"\n",
        "OUTPUT_MEL_IMG_DIR = f\"/content/drive/MyDrive/ears_data/features_mel_images/{SPEAKER_ID}\"\n",
        "\n",
        "# === Parameters ===\n",
        "SAMPLE_RATE = 16000\n",
        "N_MELS = 128\n",
        "N_CHROMA = 12\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 512\n",
        "\n",
        "# === Create output folders ===\n",
        "os.makedirs(OUTPUT_MEL_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_CHROMA_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_MEL_IMG_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"ðŸŽ§ Extracting features for: {SPEAKER_ID}\")\n",
        "\n",
        "# === Feature extraction loop ===\n",
        "for file_name in tqdm(os.listdir(AUDIO_DIR)):\n",
        "    if not file_name.endswith(\".wav\"):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(AUDIO_DIR, file_name)\n",
        "\n",
        "    # === Load audio ===\n",
        "    y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "    # === 1. Mel-Spectrogram ===\n",
        "    mel_spec = librosa.feature.melspectrogram(\n",
        "        y=y, sr=sr, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=2.0\n",
        "    )\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "    # === Save as .npy ===\n",
        "    mel_save_path = os.path.join(OUTPUT_MEL_DIR, file_name.replace(\".wav\", \"_mel.npy\"))\n",
        "    np.save(mel_save_path, mel_spec_db)\n",
        "\n",
        "    # === Save as PNG Image ===\n",
        "    mel_img_path = os.path.join(OUTPUT_MEL_IMG_DIR, file_name.replace(\".wav\", \"_mel.png\"))\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    librosa.display.specshow(\n",
        "        mel_spec_db,\n",
        "        sr=sr,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        x_axis='time',\n",
        "        y_axis='mel',\n",
        "        cmap='magma'\n",
        "    )\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(mel_img_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "    # === 2. Chroma features ===\n",
        "    chroma = librosa.feature.chroma_stft(\n",
        "        y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_chroma=N_CHROMA\n",
        "    )\n",
        "    chroma_save_path = os.path.join(OUTPUT_CHROMA_DIR, file_name.replace(\".wav\", \"_chroma.npy\"))\n",
        "    np.save(chroma_save_path, chroma)\n",
        "\n",
        "print(f\"âœ… Completed feature extraction for {SPEAKER_ID}\")\n",
        "print(f\"ðŸ“ Mel images saved to: {OUTPUT_MEL_IMG_DIR}\")\n"
      ],
      "metadata": {
        "id": "ZjI-tvw0ImqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Proposed Model( 5 emotions)**"
      ],
      "metadata": {
        "id": "swlZTRQBJhrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, MaxPooling2D,\n",
        "                                     GlobalAveragePooling2D, Dense, Dropout, Flatten,Concatenate)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "def time_mask(spec, max_mask_size=20):\n",
        "    spec = spec.copy()\n",
        "    t = spec.shape[0]\n",
        "    mask = random.randint(0, max_mask_size)\n",
        "    t0 = random.randint(0, max(1, t - mask))\n",
        "    spec[t0:t0 + mask, :] = 0\n",
        "    return spec\n",
        "\n",
        "def freq_mask(spec, max_mask_size=10):\n",
        "    spec = spec.copy()\n",
        "    f = spec.shape[1]\n",
        "    mask = random.randint(0, max_mask_size)\n",
        "    f0 = random.randint(0, max(1, f - mask))\n",
        "    spec[:, f0:f0 + mask] = 0\n",
        "    return spec\n",
        "\n",
        "def gaussian_noise(spec, noise_std=0.01):\n",
        "    return spec + np.random.normal(0, noise_std, spec.shape)\n",
        "\n",
        "def random_augment(spec):\n",
        "    \"\"\"Apply 1â€“3 random augmentations.\"\"\"\n",
        "    aug = spec.copy()\n",
        "    ops = []\n",
        "\n",
        "    # randomly choose augmentation operations\n",
        "    if random.random() < 0.7:\n",
        "        ops.append(time_mask)\n",
        "    if random.random() < 0.7:\n",
        "        ops.append(freq_mask)\n",
        "    if random.random() < 0.5:\n",
        "        ops.append(gaussian_noise)\n",
        "\n",
        "    # apply operations randomly\n",
        "    random.shuffle(ops)\n",
        "    for op in ops:\n",
        "        aug = op(aug)\n",
        "    return aug\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1ï¸âƒ£ Paths & Parameters\n",
        "# ----------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------\n",
        "# 1ï¸âƒ£ Paths & Parameters\n",
        "# ----------------------------------------------------------------------\n",
        "DATA_ROOT = '../input/spect-image/features_mel_image23' #spectogram\n",
        "npy_dir = '../input/ears-ser/mfcc_segments_23emotions' #MFCC\n",
        "selected_emotions = [\"adoration\", \"anger\", \"fear\", \"neutral\", \"sadness\"]\n",
        "\n",
        "IMG_SIZE = (128, 128)  # Resize mel images\n",
        "CHANNELS = 3           # RGB\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2ï¸âƒ£ Load matched MFCC + Mel spectrogram pairs\n",
        "# ----------------------------------------------------------------------\n",
        "mfcc_files = [f for f in os.listdir(npy_dir) if f.endswith('.npy')]\n",
        "\n",
        "mfcc_list, mel_list, labels, speakers = [], [], [], []\n",
        "missing = 0\n",
        "\n",
        "for fname in mfcc_files:\n",
        "    # Example filename:\n",
        "    # p001_emo_adoration_freeform_aug_noise_seg0.npy\n",
        "    parts = fname.replace(\".npy\", \"\").split(\"_\")\n",
        "    speaker = parts[0]\n",
        "\n",
        "    # Extract core key: emo_adoration_freeform_aug_noise\n",
        "    core_key = \"_\".join(parts[1:-1])\n",
        "\n",
        "    if \"emo_\" not in core_key:\n",
        "        continue\n",
        "\n",
        "    # Extract emotion\n",
        "    emotion = core_key.split(\"emo_\")[1].split(\"_\")[0]\n",
        "    if emotion not in selected_emotions:\n",
        "        continue\n",
        "\n",
        "    # Matching MEL image path\n",
        "    mel_name = core_key + \"_mel.png\"\n",
        "    mel_path = os.path.join(DATA_ROOT, speaker, mel_name)\n",
        "\n",
        "    if not os.path.exists(mel_path):\n",
        "        missing += 1\n",
        "        continue\n",
        "\n",
        "    # Load MFCC\n",
        "    mfcc = np.load(os.path.join(npy_dir, fname))\n",
        "\n",
        "    # Ensure MFCC shape is (time, features)\n",
        "    if mfcc.ndim == 2 and mfcc.shape[0] < mfcc.shape[1]:\n",
        "        mfcc = mfcc.T\n",
        "\n",
        "    # Load Mel image (FIXED)\n",
        "    mel_img = load_img(mel_path, target_size=IMG_SIZE)\n",
        "    mel_arr = img_to_array(mel_img) / 255.0\n",
        "\n",
        "    mfcc_list.append(mfcc)\n",
        "    mel_list.append(mel_arr)\n",
        "    labels.append(emotion)\n",
        "    speakers.append(speaker)\n",
        "\n",
        "print(f\"âœ… Loaded {len(mfcc_list)} matched samples (missing mel: {missing})\")\n",
        "\n",
        "# Convert to arrays\n",
        "X_mfcc = np.array(mfcc_list, dtype=np.float32)\n",
        "X_mel = np.array(mel_list, dtype=np.float32)\n",
        "y = np.array(labels)\n",
        "speakers = np.array(speakers)\n",
        "\n",
        "print(\"MFCC shape:\", X_mfcc.shape)\n",
        "print(\"Mel shape:\", X_mel.shape)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3ï¸âƒ£ Speaker-based Split (Same as before)\n",
        "# ----------------------------------------------------------------------\n",
        "train_speakers_male = ['p007','p008','p010','p017','p023','p040','p046','p065','p071','p076','p086','p090','p094','p095','p101']\n",
        "train_speakers_female = ['p002','p006','p024','p026','p041','p043','p047','p055','p063','p067','p080','p083','p084','p104','p098']\n",
        "val_speakers_male = ['p019','p025','p038','p039','p102']\n",
        "val_speakers_female = ['p027','p034','p037','p052','p079']\n",
        "test_speakers_male = ['p001','p074','p081','p085','p105']\n",
        "test_speakers_female = ['p012','p036','p061','p068','p073']\n",
        "\n",
        "train_speakers = train_speakers_male + train_speakers_female\n",
        "val_speakers = val_speakers_male + val_speakers_female\n",
        "test_speakers = test_speakers_male + test_speakers_female\n",
        "\n",
        "train_idx = np.isin(speakers, train_speakers)\n",
        "val_idx = np.isin(speakers, val_speakers)\n",
        "test_idx = np.isin(speakers, test_speakers)\n",
        "\n",
        "X_mfcc_train, X_mfcc_val, X_mfcc_test = X_mfcc[train_idx], X_mfcc[val_idx], X_mfcc[test_idx]\n",
        "X_mel_train, X_mel_val, X_mel_test = X_mel[train_idx], X_mel[val_idx], X_mel[test_idx]\n",
        "y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
        "\n",
        "print(f\"Train / Val / Test samples: {X_mfcc_train.shape[0]}, {X_mfcc_val.shape[0]}, {X_mfcc_test.shape[0]}\")\n",
        "print(f\"Train / Val / Test samples: {X_mel_train.shape[0]}, {X_mel_val.shape[0]}, {X_mel_test.shape[0]}\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ Standardization (Sirâ€™s Style)\n",
        "# ----------------------------------------------------------------------\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_mfcc_train_sq = np.squeeze(X_mfcc_train)\n",
        "X_mfcc_val_sq = np.squeeze(X_mfcc_val)\n",
        "X_mfcc_test_sq = np.squeeze(X_mfcc_test)\n",
        "\n",
        "scaler.fit(X_mfcc_train_sq.reshape(-1, X_mfcc_train_sq.shape[-1]))\n",
        "\n",
        "X_mfcc_train_scaled = scaler.transform(X_mfcc_train_sq.reshape(-1, X_mfcc_train_sq.shape[-1])).reshape(X_mfcc_train_sq.shape)\n",
        "X_mfcc_val_scaled = scaler.transform(X_mfcc_val_sq.reshape(-1, X_mfcc_val_sq.shape[-1])).reshape(X_mfcc_val_sq.shape)\n",
        "X_mfcc_test_scaled = scaler.transform(X_mfcc_test_sq.reshape(-1, X_mfcc_test_sq.shape[-1])).reshape(X_mfcc_test_sq.shape)\n",
        "\n",
        "X_train_st = X_mfcc_train_scaled[..., np.newaxis]\n",
        "X_val_st = X_mfcc_val_scaled[..., np.newaxis]\n",
        "X_test_st = X_mfcc_test_scaled[..., np.newaxis]\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ Apply SpecAugment-style augmentation (MFCC)\n",
        "# ----------------------------------------------------------------------\n",
        "augmented_segments_mfcc = []\n",
        "augmented_labels_mfcc = []\n",
        "aug_factor1 = 2  # number of augmented copies per sample (start with 1)\n",
        "\n",
        "print(\"Applying augmentation...\")\n",
        "\n",
        "for i in range(X_train_st.shape[0]):\n",
        "    original = X_train_st[i]\n",
        "\n",
        "    # keep original\n",
        "    augmented_segments_mfcc.append(original)\n",
        "    augmented_labels_mfcc.append(y_train[i])\n",
        "\n",
        "    # generate augmented versions\n",
        "    for _ in range(aug_factor1):\n",
        "        aug = random_augment(original)\n",
        "        augmented_segments_mfcc.append(aug)\n",
        "        augmented_labels_mfcc.append(y_train[i])\n",
        "\n",
        "# convert to numpy array\n",
        "X_train_st_aug = np.array(augmented_segments_mfcc)\n",
        "y_train_aug = np.array(augmented_labels_mfcc)\n",
        "\n",
        "print(f\"Original train samples: {X_train_st.shape[0]}\")\n",
        "print(f\"Augmented train samples: {X_train_st_aug.shape[0]}\")\n",
        "\n",
        "# Update label one-hot encoding after augmentation\n",
        "\n",
        "print(f\"\\nAfter normalization:\")\n",
        "print(f\"Train: {X_train_st.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "print(f\"\\nAfter augmentation:\")\n",
        "print(f\"Train: {X_train_st_aug.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ Apply SpecAugment-style augmentation (Mel)\n",
        "# ----------------------------------------------------------------------\n",
        "augmented_segments_mel = []\n",
        "augmented_labels_mel = []\n",
        "aug_factor2 = 2  # number of augmented copies per sample (start with 1)\n",
        "\n",
        "print(\"Applying augmentation...\")\n",
        "\n",
        "for i in range(X_mel_train.shape[0]):\n",
        "    original = X_mel_train[i]\n",
        "\n",
        "    # keep original\n",
        "    augmented_segments_mel.append(original)\n",
        "    augmented_labels_mel.append(y_train[i])\n",
        "\n",
        "    # generate augmented versions\n",
        "    for _ in range(aug_factor2):\n",
        "        aug = random_augment(original)\n",
        "        augmented_segments_mel.append(aug)\n",
        "        augmented_labels_mel.append(y_train[i])\n",
        "\n",
        "# convert to numpy array\n",
        "X_mel_train_aug = np.array(augmented_segments_mel)\n",
        "\n",
        "print(f\"Original train samples: {X_mel_train.shape[0]}\")\n",
        "print(f\"Augmented train samples: {X_mel_train_aug.shape[0]}\")\n",
        "\n",
        "# Update label one-hot encoding after augmentation\n",
        "\n",
        "print(f\"\\nAfter normalization:\")\n",
        "print(f\"Train: {X_train_st.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "print(f\"\\nAfter augmentation:\")\n",
        "print(f\"Train: {X_mel_train_aug.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4ï¸âƒ£ Label Encoding\n",
        "# ----------------------------------------------------------------------\n",
        "emotion_encoder = LabelEncoder()\n",
        "emotion_encoder.fit(np.concatenate([y_train_aug, y_val, y_test]))\n",
        "num_emotions = len(emotion_encoder.classes_)\n",
        "\n",
        "y_train_one_hot = to_categorical(emotion_encoder.transform(y_train_aug), num_classes=num_emotions)\n",
        "y_val_one_hot = to_categorical(emotion_encoder.transform(y_val), num_classes=num_emotions)\n",
        "y_test_one_hot = to_categorical(emotion_encoder.transform(y_test), num_classes=num_emotions)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ 2D CNN Model\n",
        "# ----------------------------------------------------------------------\n",
        "def mfcc_branch(input_shape, l2_lambda=0.01):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv2D(32, (3,3), activation='relu', padding='same',\n",
        "               kernel_regularizer=l2(l2_lambda))(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same',\n",
        "               kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same',\n",
        "               kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    return inputs, x\n",
        "\n",
        "\n",
        "def mel_branch(input_shape, l2_lambda=0.01):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    y = Conv2D(32, (3,3), activation='relu', padding='same', kernel_regularizer=l2(l2_lambda))(inputs)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling2D((2,2))(y)\n",
        "\n",
        "    y = Conv2D(64, (3,3), activation='relu', padding='same', kernel_regularizer=l2(l2_lambda))(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling2D((2,2))(y)\n",
        "\n",
        "    y = Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=l2(l2_lambda))(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = GlobalAveragePooling2D()(y)\n",
        "    y = Dropout(0.2)(y)\n",
        "    y = Dense(64, activation='relu', kernel_regularizer=l2(l2_lambda))(y)\n",
        "    y = Dropout(0.5)(y)\n",
        "    return inputs, y\n",
        "\n",
        "mfcc_in, mfcc_feat = mfcc_branch(X_train_st_aug.shape[1:])\n",
        "# Fix the shape for the Mel spectrogram input\n",
        "mel_in, mel_feat = mel_branch(X_mel_train_aug.shape[1:])  # Remove the last dimension in shape\n",
        "\n",
        "# Now you can safely concatenate the two branches\n",
        "merged = Concatenate()([mfcc_feat, mel_feat])\n",
        "x = Dense(128, activation='relu')(merged)\n",
        "x = Dropout(0.5)(x)\n",
        "out = Dense(num_emotions, activation='softmax')(x)\n",
        "model = Model([mfcc_in, mel_in], out)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 6ï¸âƒ£ Train\n",
        "# ----------------------------------------------------------------------\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "\n",
        "    [X_train_st_aug, X_mel_train_aug],\n",
        "    y_train_one_hot,\n",
        "    validation_data=([X_val_st, X_mel_val], y_val_one_hot),\n",
        "    batch_size=32,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 7ï¸âƒ£ Separate Testing by Gender\n",
        "# ----------------------------------------------------------------------\n",
        "gender_map = {spk:0 for spk in (train_speakers_male + val_speakers_male + test_speakers_male)}\n",
        "gender_map.update({spk:1 for spk in (train_speakers_female + val_speakers_female + test_speakers_female)})\n",
        "\n",
        "y_test_gender = np.array([gender_map[spk] for spk in speakers[test_idx]])\n",
        "y_true = np.argmax(y_test_one_hot, axis=1)\n",
        "y_pred_probs = model.predict([X_test_st, X_mel_test])\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "male_mask = (y_test_gender == 0)\n",
        "female_mask = (y_test_gender == 1)\n",
        "\n",
        "# ---- Male\n",
        "y_true_male = y_true[male_mask]\n",
        "y_pred_male = y_pred[male_mask]\n",
        "male_acc = np.mean(y_pred_male == y_true_male)\n",
        "male_precision = precision_score(y_true_male, y_pred_male, average='macro', zero_division=0)\n",
        "male_recall = recall_score(y_true_male, y_pred_male, average='macro', zero_division=0)\n",
        "male_f1 = f1_score(y_true_male, y_pred_male, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n--- ðŸ‘¨ Male Test Results ---\")\n",
        "print(f\"Accuracy : {male_acc*100:.2f}%\")\n",
        "print(f\"Precision: {male_precision*100:.2f}%\")\n",
        "print(f\"Recall   : {male_recall*100:.2f}%\")\n",
        "print(f\"F1-score : {male_f1*100:.2f}%\")\n",
        "print(classification_report(y_true_male, y_pred_male, target_names=emotion_encoder.classes_))\n",
        "cm_male = confusion_matrix(y_true_male, y_pred_male)\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm_male, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)\n",
        "plt.title(\"ðŸ‘¨ Confusion Matrix â€“ Male Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# ---- Female\n",
        "y_true_female = y_true[female_mask]\n",
        "y_pred_female = y_pred[female_mask]\n",
        "female_acc = np.mean(y_pred_female == y_true_female)\n",
        "female_precision = precision_score(y_true_female, y_pred_female, average='macro', zero_division=0)\n",
        "female_recall = recall_score(y_true_female, y_pred_female, average='macro', zero_division=0)\n",
        "female_f1 = f1_score(y_true_female, y_pred_female, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n--- ðŸ‘© Female Test Results ---\")\n",
        "print(f\"Accuracy : {female_acc*100:.2f}%\")\n",
        "print(f\"Precision: {female_precision*100:.2f}%\")\n",
        "print(f\"Recall   : {female_recall*100:.2f}%\")\n",
        "print(f\"F1-score : {female_f1*100:.2f}%\")\n",
        "print(classification_report(y_true_female, y_pred_female, target_names=emotion_encoder.classes_))\n",
        "cm_female = confusion_matrix(y_true_female, y_pred_female)\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm_female, annot=True, fmt='d', cmap='Reds',\n",
        "            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)\n",
        "plt.title(\"ðŸ‘© Confusion Matrix â€“ Female Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# ---- Difference\n",
        "diff_acc = male_acc - female_acc\n",
        "print(\"\\n--- ðŸ” Performance Difference (Male - Female) ---\")\n",
        "print(f\"Accuracy Difference : {diff_acc*100:.2f}%\")\n",
        "print(f\"Absolute Accuracy Gap : {abs(diff_acc)*100:.2f}%\")\n",
        "\n",
        "# ---- Overall\n",
        "overall_acc = np.mean(y_pred == y_true)\n",
        "overall_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "overall_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "overall_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n--- âš–ï¸ Overall Combined Results ---\")\n",
        "print(f\"Overall Accuracy : {overall_acc*100:.2f}%\")\n",
        "print(f\"Overall Precision: {overall_precision*100:.2f}%\")\n",
        "print(f\"Overall Recall   : {overall_recall*100:.2f}%\")\n",
        "print(f\"Overall F1-score : {overall_f1*100:.2f}%\")\n",
        "cm_overall = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_overall, annot=True, fmt='d', cmap='Purples',\n",
        "            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)\n",
        "plt.title(\"âš–ï¸ Confusion Matrix â€“ Combined Male + Female Test Set\")\n",
        "plt.show()\n",
        "# -------------------------------\n",
        "# 4ï¸âƒ£ Plot Accuracy and Loss Curves\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CCFXXUSBJnj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Proposed Model(7 emotions)**"
      ],
      "metadata": {
        "id": "RUfJwIn3J9fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, MaxPooling2D,\n",
        "                                     GlobalAveragePooling2D, Dense, Dropout, Flatten,Concatenate)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import random\n",
        "\n",
        "def time_mask(spec, max_mask_size=20):\n",
        "    spec = spec.copy()\n",
        "    t = spec.shape[0]\n",
        "    mask = random.randint(0, max_mask_size)\n",
        "    t0 = random.randint(0, max(1, t - mask))\n",
        "    spec[t0:t0 + mask, :] = 0\n",
        "    return spec\n",
        "\n",
        "def freq_mask(spec, max_mask_size=10):\n",
        "    spec = spec.copy()\n",
        "    f = spec.shape[1]\n",
        "    mask = random.randint(0, max_mask_size)\n",
        "    f0 = random.randint(0, max(1, f - mask))\n",
        "    spec[:, f0:f0 + mask] = 0\n",
        "    return spec\n",
        "\n",
        "def gaussian_noise(spec, noise_std=0.01):\n",
        "    return spec + np.random.normal(0, noise_std, spec.shape)\n",
        "\n",
        "def random_augment(spec):\n",
        "    \"\"\"Apply 1â€“3 random augmentations.\"\"\"\n",
        "    aug = spec.copy()\n",
        "    ops = []\n",
        "\n",
        "    # randomly choose augmentation operations\n",
        "    if random.random() < 0.7:\n",
        "        ops.append(time_mask)\n",
        "    if random.random() < 0.7:\n",
        "        ops.append(freq_mask)\n",
        "    if random.random() < 0.5:\n",
        "        ops.append(gaussian_noise)\n",
        "\n",
        "    # apply operations randomly\n",
        "    random.shuffle(ops)\n",
        "    for op in ops:\n",
        "        aug = op(aug)\n",
        "    return aug\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1ï¸âƒ£ Paths & Parameters\n",
        "# ----------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------\n",
        "# 1ï¸âƒ£ Paths & Parameters\n",
        "# ----------------------------------------------------------------------\n",
        "DATA_ROOT = '../input/spect-image/features_mel_image23'\n",
        "npy_dir = '../input/ears-ser/mfcc_segments_23emotions'\n",
        "selected_emotions = [\"adoration\", \"anger\", \"fear\", \"neutral\", \"sadness\", \"disappointment\",\"pain\"]\n",
        "\n",
        "IMG_SIZE = (128, 128)  # Resize mel images\n",
        "CHANNELS = 3           # RGB\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2ï¸âƒ£ Load matched MFCC + Mel spectrogram pairs\n",
        "# ----------------------------------------------------------------------\n",
        "mfcc_files = [f for f in os.listdir(npy_dir) if f.endswith('.npy')]\n",
        "\n",
        "mfcc_list, mel_list, labels, speakers = [], [], [], []\n",
        "missing = 0\n",
        "\n",
        "for fname in mfcc_files:\n",
        "    # Example filename:\n",
        "    # p001_emo_adoration_freeform_aug_noise_seg0.npy\n",
        "    parts = fname.replace(\".npy\", \"\").split(\"_\")\n",
        "    speaker = parts[0]\n",
        "\n",
        "    # Extract core key: emo_adoration_freeform_aug_noise\n",
        "    core_key = \"_\".join(parts[1:-1])\n",
        "\n",
        "    if \"emo_\" not in core_key:\n",
        "        continue\n",
        "\n",
        "    # Extract emotion\n",
        "    emotion = core_key.split(\"emo_\")[1].split(\"_\")[0]\n",
        "    if emotion not in selected_emotions:\n",
        "        continue\n",
        "\n",
        "    # Matching MEL image path\n",
        "    mel_name = core_key + \"_mel.png\"\n",
        "    mel_path = os.path.join(DATA_ROOT, speaker, mel_name)\n",
        "\n",
        "    if not os.path.exists(mel_path):\n",
        "        missing += 1\n",
        "        continue\n",
        "\n",
        "    # Load MFCC\n",
        "    mfcc = np.load(os.path.join(npy_dir, fname))\n",
        "\n",
        "    # Ensure MFCC shape is (time, features)\n",
        "    if mfcc.ndim == 2 and mfcc.shape[0] < mfcc.shape[1]:\n",
        "        mfcc = mfcc.T\n",
        "\n",
        "    # Load Mel image (FIXED)\n",
        "    mel_img = load_img(mel_path, target_size=IMG_SIZE)\n",
        "    mel_arr = img_to_array(mel_img) / 255.0\n",
        "\n",
        "    mfcc_list.append(mfcc)\n",
        "    mel_list.append(mel_arr)\n",
        "    labels.append(emotion)\n",
        "    speakers.append(speaker)\n",
        "\n",
        "print(f\"âœ… Loaded {len(mfcc_list)} matched samples (missing mel: {missing})\")\n",
        "\n",
        "# Convert to arrays\n",
        "X_mfcc = np.array(mfcc_list, dtype=np.float32)\n",
        "X_mel = np.array(mel_list, dtype=np.float32)\n",
        "y = np.array(labels)\n",
        "speakers = np.array(speakers)\n",
        "\n",
        "print(\"MFCC shape:\", X_mfcc.shape)\n",
        "print(\"Mel shape:\", X_mel.shape)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3ï¸âƒ£ Speaker-based Split (Same as before)\n",
        "# ----------------------------------------------------------------------\n",
        "train_speakers_male = ['p007','p008','p010','p017','p023','p040','p046','p065','p071','p076','p086','p090','p094','p095','p101']\n",
        "train_speakers_female = ['p002','p006','p024','p026','p041','p043','p047','p055','p063','p067','p080','p083','p084','p104','p098']\n",
        "val_speakers_male = ['p019','p025','p038','p039','p102']\n",
        "val_speakers_female = ['p027','p034','p037','p052','p079']\n",
        "test_speakers_male = ['p001','p074','p081','p085','p105']\n",
        "test_speakers_female = ['p012','p036','p061','p068','p073']\n",
        "\n",
        "train_speakers = train_speakers_male + train_speakers_female\n",
        "val_speakers = val_speakers_male + val_speakers_female\n",
        "test_speakers = test_speakers_male + test_speakers_female\n",
        "\n",
        "train_idx = np.isin(speakers, train_speakers)\n",
        "val_idx = np.isin(speakers, val_speakers)\n",
        "test_idx = np.isin(speakers, test_speakers)\n",
        "\n",
        "X_mfcc_train, X_mfcc_val, X_mfcc_test = X_mfcc[train_idx], X_mfcc[val_idx], X_mfcc[test_idx]\n",
        "X_mel_train, X_mel_val, X_mel_test = X_mel[train_idx], X_mel[val_idx], X_mel[test_idx]\n",
        "y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
        "\n",
        "print(f\"Train / Val / Test samples: {X_mfcc_train.shape[0]}, {X_mfcc_val.shape[0]}, {X_mfcc_test.shape[0]}\")\n",
        "print(f\"Train / Val / Test samples: {X_mel_train.shape[0]}, {X_mel_val.shape[0]}, {X_mel_test.shape[0]}\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ Standardization (Sirâ€™s Style)\n",
        "# ----------------------------------------------------------------------\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_mfcc_train_sq = np.squeeze(X_mfcc_train)\n",
        "X_mfcc_val_sq = np.squeeze(X_mfcc_val)\n",
        "X_mfcc_test_sq = np.squeeze(X_mfcc_test)\n",
        "\n",
        "scaler.fit(X_mfcc_train_sq.reshape(-1, X_mfcc_train_sq.shape[-1]))\n",
        "\n",
        "X_mfcc_train_scaled = scaler.transform(X_mfcc_train_sq.reshape(-1, X_mfcc_train_sq.shape[-1])).reshape(X_mfcc_train_sq.shape)\n",
        "X_mfcc_val_scaled = scaler.transform(X_mfcc_val_sq.reshape(-1, X_mfcc_val_sq.shape[-1])).reshape(X_mfcc_val_sq.shape)\n",
        "X_mfcc_test_scaled = scaler.transform(X_mfcc_test_sq.reshape(-1, X_mfcc_test_sq.shape[-1])).reshape(X_mfcc_test_sq.shape)\n",
        "\n",
        "X_train_st = X_mfcc_train_scaled[..., np.newaxis]\n",
        "X_val_st = X_mfcc_val_scaled[..., np.newaxis]\n",
        "X_test_st = X_mfcc_test_scaled[..., np.newaxis]\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ Apply SpecAugment-style augmentation (MFCC)\n",
        "# ----------------------------------------------------------------------\n",
        "augmented_segments_mfcc = []\n",
        "augmented_labels_mfcc = []\n",
        "aug_factor1 = 2  # number of augmented copies per sample (start with 1)\n",
        "\n",
        "print(\"Applying augmentation...\")\n",
        "\n",
        "for i in range(X_train_st.shape[0]):\n",
        "    original = X_train_st[i]\n",
        "\n",
        "    # keep original\n",
        "    augmented_segments_mfcc.append(original)\n",
        "    augmented_labels_mfcc.append(y_train[i])\n",
        "\n",
        "    # generate augmented versions\n",
        "    for _ in range(aug_factor1):\n",
        "        aug = random_augment(original)\n",
        "        augmented_segments_mfcc.append(aug)\n",
        "        augmented_labels_mfcc.append(y_train[i])\n",
        "\n",
        "# convert to numpy array\n",
        "X_train_st_aug = np.array(augmented_segments_mfcc)\n",
        "y_train_aug = np.array(augmented_labels_mfcc)\n",
        "\n",
        "print(f\"Original train samples: {X_train_st.shape[0]}\")\n",
        "print(f\"Augmented train samples: {X_train_st_aug.shape[0]}\")\n",
        "\n",
        "# Update label one-hot encoding after augmentation\n",
        "\n",
        "print(f\"\\nAfter normalization:\")\n",
        "print(f\"Train: {X_train_st.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "print(f\"\\nAfter augmentation:\")\n",
        "print(f\"Train: {X_train_st_aug.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ Apply SpecAugment-style augmentation (Mel)\n",
        "# ----------------------------------------------------------------------\n",
        "augmented_segments_mel = []\n",
        "augmented_labels_mel = []\n",
        "aug_factor2 = 2  # number of augmented copies per sample (start with 1)\n",
        "\n",
        "print(\"Applying augmentation...\")\n",
        "\n",
        "for i in range(X_mel_train.shape[0]):\n",
        "    original = X_mel_train[i]\n",
        "\n",
        "    # keep original\n",
        "    augmented_segments_mel.append(original)\n",
        "    augmented_labels_mel.append(y_train[i])\n",
        "\n",
        "    # generate augmented versions\n",
        "    for _ in range(aug_factor2):\n",
        "        aug = random_augment(original)\n",
        "        augmented_segments_mel.append(aug)\n",
        "        augmented_labels_mel.append(y_train[i])\n",
        "\n",
        "# convert to numpy array\n",
        "X_mel_train_aug = np.array(augmented_segments_mel)\n",
        "\n",
        "print(f\"Original train samples: {X_mel_train.shape[0]}\")\n",
        "print(f\"Augmented train samples: {X_mel_train_aug.shape[0]}\")\n",
        "\n",
        "# Update label one-hot encoding after augmentation\n",
        "\n",
        "print(f\"\\nAfter normalization:\")\n",
        "print(f\"Train: {X_train_st.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "print(f\"\\nAfter augmentation:\")\n",
        "print(f\"Train: {X_mel_train_aug.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4ï¸âƒ£ Label Encoding\n",
        "# ----------------------------------------------------------------------\n",
        "emotion_encoder = LabelEncoder()\n",
        "emotion_encoder.fit(np.concatenate([y_train_aug, y_val, y_test]))\n",
        "num_emotions = len(emotion_encoder.classes_)\n",
        "\n",
        "y_train_one_hot = to_categorical(emotion_encoder.transform(y_train_aug), num_classes=num_emotions)\n",
        "y_val_one_hot = to_categorical(emotion_encoder.transform(y_val), num_classes=num_emotions)\n",
        "y_test_one_hot = to_categorical(emotion_encoder.transform(y_test), num_classes=num_emotions)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ 2D CNN Model\n",
        "# ----------------------------------------------------------------------\n",
        "def mfcc_branch(input_shape, l2_lambda=0.01):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv2D(32, (3,3), activation='relu', padding='same',\n",
        "               kernel_regularizer=l2(l2_lambda))(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same',\n",
        "               kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same',\n",
        "               kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    return inputs, x\n",
        "\n",
        "\n",
        "def mel_branch(input_shape, l2_lambda=0.01):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    y = Conv2D(32, (3,3), activation='relu', padding='same', kernel_regularizer=l2(l2_lambda))(inputs)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling2D((2,2))(y)\n",
        "\n",
        "    y = Conv2D(64, (3,3), activation='relu', padding='same', kernel_regularizer=l2(l2_lambda))(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling2D((2,2))(y)\n",
        "\n",
        "    y = Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=l2(l2_lambda))(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = GlobalAveragePooling2D()(y)\n",
        "    y = Dropout(0.2)(y)\n",
        "    y = Dense(64, activation='relu', kernel_regularizer=l2(l2_lambda))(y)\n",
        "    y = Dropout(0.5)(y)\n",
        "    return inputs, y\n",
        "\n",
        "mfcc_in, mfcc_feat = mfcc_branch(X_train_st_aug.shape[1:])\n",
        "# Fix the shape for the Mel spectrogram input\n",
        "mel_in, mel_feat = mel_branch(X_mel_train_aug.shape[1:])  # Remove the last dimension in shape\n",
        "\n",
        "# Now you can safely concatenate the two branches\n",
        "merged = Concatenate()([mfcc_feat, mel_feat])\n",
        "x = Dense(128, activation='relu')(merged)\n",
        "x = Dropout(0.5)(x)\n",
        "out = Dense(num_emotions, activation='softmax')(x)\n",
        "model = Model([mfcc_in, mel_in], out)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 6ï¸âƒ£ Train\n",
        "# ----------------------------------------------------------------------\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "\n",
        "    [X_train_st_aug, X_mel_train_aug],\n",
        "    y_train_one_hot,\n",
        "    validation_data=([X_val_st, X_mel_val], y_val_one_hot),\n",
        "    batch_size=32,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 7ï¸âƒ£ Separate Testing by Gender\n",
        "# ----------------------------------------------------------------------\n",
        "gender_map = {spk:0 for spk in (train_speakers_male + val_speakers_male + test_speakers_male)}\n",
        "gender_map.update({spk:1 for spk in (train_speakers_female + val_speakers_female + test_speakers_female)})\n",
        "\n",
        "y_test_gender = np.array([gender_map[spk] for spk in speakers[test_idx]])\n",
        "y_true = np.argmax(y_test_one_hot, axis=1)\n",
        "y_pred_probs = model.predict([X_test_st, X_mel_test])\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "male_mask = (y_test_gender == 0)\n",
        "female_mask = (y_test_gender == 1)\n",
        "\n",
        "# ---- Male\n",
        "y_true_male = y_true[male_mask]\n",
        "y_pred_male = y_pred[male_mask]\n",
        "male_acc = np.mean(y_pred_male == y_true_male)\n",
        "male_precision = precision_score(y_true_male, y_pred_male, average='macro', zero_division=0)\n",
        "male_recall = recall_score(y_true_male, y_pred_male, average='macro', zero_division=0)\n",
        "male_f1 = f1_score(y_true_male, y_pred_male, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n--- ðŸ‘¨ Male Test Results ---\")\n",
        "print(f\"Accuracy : {male_acc*100:.2f}%\")\n",
        "print(f\"Precision: {male_precision*100:.2f}%\")\n",
        "print(f\"Recall   : {male_recall*100:.2f}%\")\n",
        "print(f\"F1-score : {male_f1*100:.2f}%\")\n",
        "print(classification_report(y_true_male, y_pred_male, target_names=emotion_encoder.classes_))\n",
        "cm_male = confusion_matrix(y_true_male, y_pred_male)\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm_male, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)\n",
        "plt.title(\"ðŸ‘¨ Confusion Matrix â€“ Male Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# ---- Female\n",
        "y_true_female = y_true[female_mask]\n",
        "y_pred_female = y_pred[female_mask]\n",
        "female_acc = np.mean(y_pred_female == y_true_female)\n",
        "female_precision = precision_score(y_true_female, y_pred_female, average='macro', zero_division=0)\n",
        "female_recall = recall_score(y_true_female, y_pred_female, average='macro', zero_division=0)\n",
        "female_f1 = f1_score(y_true_female, y_pred_female, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n--- ðŸ‘© Female Test Results ---\")\n",
        "print(f\"Accuracy : {female_acc*100:.2f}%\")\n",
        "print(f\"Precision: {female_precision*100:.2f}%\")\n",
        "print(f\"Recall   : {female_recall*100:.2f}%\")\n",
        "print(f\"F1-score : {female_f1*100:.2f}%\")\n",
        "print(classification_report(y_true_female, y_pred_female, target_names=emotion_encoder.classes_))\n",
        "cm_female = confusion_matrix(y_true_female, y_pred_female)\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm_female, annot=True, fmt='d', cmap='Reds',\n",
        "            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)\n",
        "plt.title(\"ðŸ‘© Confusion Matrix â€“ Female Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# ---- Difference\n",
        "diff_acc = male_acc - female_acc\n",
        "print(\"\\n--- ðŸ” Performance Difference (Male - Female) ---\")\n",
        "print(f\"Accuracy Difference : {diff_acc*100:.2f}%\")\n",
        "print(f\"Absolute Accuracy Gap : {abs(diff_acc)*100:.2f}%\")\n",
        "\n",
        "# ---- Overall\n",
        "overall_acc = np.mean(y_pred == y_true)\n",
        "overall_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "overall_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "overall_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n--- âš–ï¸ Overall Combined Results ---\")\n",
        "print(f\"Overall Accuracy : {overall_acc*100:.2f}%\")\n",
        "print(f\"Overall Precision: {overall_precision*100:.2f}%\")\n",
        "print(f\"Overall Recall   : {overall_recall*100:.2f}%\")\n",
        "print(f\"Overall F1-score : {overall_f1*100:.2f}%\")\n",
        "cm_overall = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_overall, annot=True, fmt='d', cmap='Purples',\n",
        "            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)\n",
        "plt.title(\"âš–ï¸ Confusion Matrix â€“ Combined Male + Female Test Set\")\n",
        "plt.show()\n",
        "# -------------------------------\n",
        "# 4ï¸âƒ£ Plot Accuracy and Loss Curves\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0qtJCVR-J7BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Proposed Model(23 emotions)**"
      ],
      "metadata": {
        "id": "VHGjT3PeKMgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, MaxPooling2D,\n",
        "                                     GlobalAveragePooling2D, Dense, Dropout, Flatten,Concatenate)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "def time_mask(spec, max_mask_size=20):\n",
        "    spec = spec.copy()\n",
        "    t = spec.shape[0]\n",
        "    mask = random.randint(0, max_mask_size)\n",
        "    t0 = random.randint(0, max(1, t - mask))\n",
        "    spec[t0:t0 + mask, :] = 0\n",
        "    return spec\n",
        "\n",
        "def freq_mask(spec, max_mask_size=10):\n",
        "    spec = spec.copy()\n",
        "    f = spec.shape[1]\n",
        "    mask = random.randint(0, max_mask_size)\n",
        "    f0 = random.randint(0, max(1, f - mask))\n",
        "    spec[:, f0:f0 + mask] = 0\n",
        "    return spec\n",
        "\n",
        "def gaussian_noise(spec, noise_std=0.01):\n",
        "    return spec + np.random.normal(0, noise_std, spec.shape)\n",
        "\n",
        "def random_augment(spec):\n",
        "    \"\"\"Apply 1â€“3 random augmentations.\"\"\"\n",
        "    aug = spec.copy()\n",
        "    ops = []\n",
        "\n",
        "    # randomly choose augmentation operations\n",
        "    if random.random() < 0.7:\n",
        "        ops.append(time_mask)\n",
        "    if random.random() < 0.7:\n",
        "        ops.append(freq_mask)\n",
        "    if random.random() < 0.5:\n",
        "        ops.append(gaussian_noise)\n",
        "\n",
        "    # apply operations randomly\n",
        "    random.shuffle(ops)\n",
        "    for op in ops:\n",
        "        aug = op(aug)\n",
        "    return aug\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1ï¸âƒ£ Paths & Parameters\n",
        "# ----------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------\n",
        "# 1ï¸âƒ£ Paths & Parameters\n",
        "# ----------------------------------------------------------------------\n",
        "DATA_ROOT = '../input/spect-image/features_mel_image23'\n",
        "npy_dir = '../input/ears-ser/mfcc_segments_23emotions'\n",
        "selected_emotions = [\"adoration\", \"anger\", \"amazement\", \"amusement\", \"confusion\",\"contentment\", \"cuteness\", \"desire\", \"disappointment\", \"disgust\",\"distress\", \"embarassment\",\"extasy\", \"fear\", \"guilt\",\"interest\", \"neutral\", \"sadness\",\"pain\",\"pride\",\"realization\",\"relief\", \"serenity\"]\n",
        "\n",
        "IMG_SIZE = (128, 128)  # Resize mel images\n",
        "CHANNELS = 3           # RGB\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2ï¸âƒ£ Load matched MFCC + Mel spectrogram pairs\n",
        "# ----------------------------------------------------------------------\n",
        "mfcc_files = [f for f in os.listdir(npy_dir) if f.endswith('.npy')]\n",
        "\n",
        "mfcc_list, mel_list, labels, speakers = [], [], [], []\n",
        "missing = 0\n",
        "\n",
        "for fname in mfcc_files:\n",
        "    # Example filename:\n",
        "    # p001_emo_adoration_freeform_aug_noise_seg0.npy\n",
        "    parts = fname.replace(\".npy\", \"\").split(\"_\")\n",
        "    speaker = parts[0]\n",
        "\n",
        "    # Extract core key: emo_adoration_freeform_aug_noise\n",
        "    core_key = \"_\".join(parts[1:-1])\n",
        "\n",
        "    if \"emo_\" not in core_key:\n",
        "        continue\n",
        "\n",
        "    # Extract emotion\n",
        "    emotion = core_key.split(\"emo_\")[1].split(\"_\")[0]\n",
        "    if emotion not in selected_emotions:\n",
        "        continue\n",
        "\n",
        "    # Matching MEL image path\n",
        "    mel_name = core_key + \"_mel.png\"\n",
        "    mel_path = os.path.join(DATA_ROOT, speaker, mel_name)\n",
        "\n",
        "    if not os.path.exists(mel_path):\n",
        "        missing += 1\n",
        "        continue\n",
        "\n",
        "    # Load MFCC\n",
        "    mfcc = np.load(os.path.join(npy_dir, fname))\n",
        "\n",
        "    # Ensure MFCC shape is (time, features)\n",
        "    if mfcc.ndim == 2 and mfcc.shape[0] < mfcc.shape[1]:\n",
        "        mfcc = mfcc.T\n",
        "\n",
        "    # Load Mel image (FIXED)\n",
        "    mel_img = load_img(mel_path, target_size=IMG_SIZE)\n",
        "    mel_arr = img_to_array(mel_img) / 255.0\n",
        "\n",
        "    mfcc_list.append(mfcc)\n",
        "    mel_list.append(mel_arr)\n",
        "    labels.append(emotion)\n",
        "    speakers.append(speaker)\n",
        "\n",
        "print(f\"âœ… Loaded {len(mfcc_list)} matched samples (missing mel: {missing})\")\n",
        "\n",
        "# Convert to arrays\n",
        "X_mfcc = np.array(mfcc_list, dtype=np.float32)\n",
        "X_mel = np.array(mel_list, dtype=np.float32)\n",
        "y = np.array(labels)\n",
        "speakers = np.array(speakers)\n",
        "\n",
        "print(\"MFCC shape:\", X_mfcc.shape)\n",
        "print(\"Mel shape:\", X_mel.shape)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3ï¸âƒ£ Speaker-based Split (Same as before)\n",
        "# ----------------------------------------------------------------------\n",
        "train_speakers_male = ['p007','p008','p010','p017','p023','p040','p046','p065','p071','p076','p086','p090','p094','p095','p101']\n",
        "train_speakers_female = ['p002','p006','p024','p026','p041','p043','p047','p055','p063','p067','p080','p083','p084','p104','p098']\n",
        "val_speakers_male = ['p019','p025','p038','p039','p102']\n",
        "val_speakers_female = ['p027','p034','p037','p052','p079']\n",
        "test_speakers_male = ['p001','p074','p081','p085','p105']\n",
        "test_speakers_female = ['p012','p036','p061','p068','p073']\n",
        "\n",
        "train_speakers = train_speakers_male + train_speakers_female\n",
        "val_speakers = val_speakers_male + val_speakers_female\n",
        "test_speakers = test_speakers_male + test_speakers_female\n",
        "\n",
        "train_idx = np.isin(speakers, train_speakers)\n",
        "val_idx = np.isin(speakers, val_speakers)\n",
        "test_idx = np.isin(speakers, test_speakers)\n",
        "\n",
        "X_mfcc_train, X_mfcc_val, X_mfcc_test = X_mfcc[train_idx], X_mfcc[val_idx], X_mfcc[test_idx]\n",
        "X_mel_train, X_mel_val, X_mel_test = X_mel[train_idx], X_mel[val_idx], X_mel[test_idx]\n",
        "y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
        "\n",
        "print(f\"Train / Val / Test samples: {X_mfcc_train.shape[0]}, {X_mfcc_val.shape[0]}, {X_mfcc_test.shape[0]}\")\n",
        "print(f\"Train / Val / Test samples: {X_mel_train.shape[0]}, {X_mel_val.shape[0]}, {X_mel_test.shape[0]}\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ Standardization (Sirâ€™s Style)\n",
        "# ----------------------------------------------------------------------\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_mfcc_train_sq = np.squeeze(X_mfcc_train)\n",
        "X_mfcc_val_sq = np.squeeze(X_mfcc_val)\n",
        "X_mfcc_test_sq = np.squeeze(X_mfcc_test)\n",
        "\n",
        "scaler.fit(X_mfcc_train_sq.reshape(-1, X_mfcc_train_sq.shape[-1]))\n",
        "\n",
        "X_mfcc_train_scaled = scaler.transform(X_mfcc_train_sq.reshape(-1, X_mfcc_train_sq.shape[-1])).reshape(X_mfcc_train_sq.shape)\n",
        "X_mfcc_val_scaled = scaler.transform(X_mfcc_val_sq.reshape(-1, X_mfcc_val_sq.shape[-1])).reshape(X_mfcc_val_sq.shape)\n",
        "X_mfcc_test_scaled = scaler.transform(X_mfcc_test_sq.reshape(-1, X_mfcc_test_sq.shape[-1])).reshape(X_mfcc_test_sq.shape)\n",
        "\n",
        "X_train_st = X_mfcc_train_scaled[..., np.newaxis]\n",
        "X_val_st = X_mfcc_val_scaled[..., np.newaxis]\n",
        "X_test_st = X_mfcc_test_scaled[..., np.newaxis]\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ Apply SpecAugment-style augmentation (MFCC)\n",
        "# ----------------------------------------------------------------------\n",
        "augmented_segments_mfcc = []\n",
        "augmented_labels_mfcc = []\n",
        "aug_factor1 = 2  # number of augmented copies per sample (start with 1)\n",
        "\n",
        "print(\"Applying augmentation...\")\n",
        "\n",
        "for i in range(X_train_st.shape[0]):\n",
        "    original = X_train_st[i]\n",
        "\n",
        "    # keep original\n",
        "    augmented_segments_mfcc.append(original)\n",
        "    augmented_labels_mfcc.append(y_train[i])\n",
        "\n",
        "    # generate augmented versions\n",
        "    for _ in range(aug_factor1):\n",
        "        aug = random_augment(original)\n",
        "        augmented_segments_mfcc.append(aug)\n",
        "        augmented_labels_mfcc.append(y_train[i])\n",
        "\n",
        "# convert to numpy array\n",
        "X_train_st_aug = np.array(augmented_segments_mfcc)\n",
        "y_train_aug = np.array(augmented_labels_mfcc)\n",
        "\n",
        "print(f\"Original train samples: {X_train_st.shape[0]}\")\n",
        "print(f\"Augmented train samples: {X_train_st_aug.shape[0]}\")\n",
        "\n",
        "# Update label one-hot encoding after augmentation\n",
        "\n",
        "print(f\"\\nAfter normalization:\")\n",
        "print(f\"Train: {X_train_st.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "print(f\"\\nAfter augmentation:\")\n",
        "print(f\"Train: {X_train_st_aug.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ Apply SpecAugment-style augmentation (Mel)\n",
        "# ----------------------------------------------------------------------\n",
        "augmented_segments_mel = []\n",
        "augmented_labels_mel = []\n",
        "aug_factor2 = 2  # number of augmented copies per sample (start with 1)\n",
        "\n",
        "print(\"Applying augmentation...\")\n",
        "\n",
        "for i in range(X_mel_train.shape[0]):\n",
        "    original = X_mel_train[i]\n",
        "\n",
        "    # keep original\n",
        "    augmented_segments_mel.append(original)\n",
        "    augmented_labels_mel.append(y_train[i])\n",
        "\n",
        "    # generate augmented versions\n",
        "    for _ in range(aug_factor2):\n",
        "        aug = random_augment(original)\n",
        "        augmented_segments_mel.append(aug)\n",
        "        augmented_labels_mel.append(y_train[i])\n",
        "\n",
        "# convert to numpy array\n",
        "X_mel_train_aug = np.array(augmented_segments_mel)\n",
        "\n",
        "print(f\"Original train samples: {X_mel_train.shape[0]}\")\n",
        "print(f\"Augmented train samples: {X_mel_train_aug.shape[0]}\")\n",
        "\n",
        "# Update label one-hot encoding after augmentation\n",
        "\n",
        "print(f\"\\nAfter normalization:\")\n",
        "print(f\"Train: {X_train_st.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "print(f\"\\nAfter augmentation:\")\n",
        "print(f\"Train: {X_mel_train_aug.shape}, Val: {X_val_st.shape}, Test: {X_test_st.shape}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4ï¸âƒ£ Label Encoding\n",
        "# ----------------------------------------------------------------------\n",
        "emotion_encoder = LabelEncoder()\n",
        "emotion_encoder.fit(np.concatenate([y_train_aug, y_val, y_test]))\n",
        "num_emotions = len(emotion_encoder.classes_)\n",
        "\n",
        "y_train_one_hot = to_categorical(emotion_encoder.transform(y_train_aug), num_classes=num_emotions)\n",
        "y_val_one_hot = to_categorical(emotion_encoder.transform(y_val), num_classes=num_emotions)\n",
        "y_test_one_hot = to_categorical(emotion_encoder.transform(y_test), num_classes=num_emotions)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5ï¸âƒ£ 2D CNN Model\n",
        "# ----------------------------------------------------------------------\n",
        "def mfcc_branch(input_shape, l2_lambda=0.01):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv2D(32, (3,3), activation='relu', padding='same',\n",
        "               kernel_regularizer=l2(l2_lambda))(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same',\n",
        "               kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same',\n",
        "               kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    return inputs, x\n",
        "\n",
        "\n",
        "def mel_branch(input_shape, l2_lambda=0.01):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    y = Conv2D(32, (3,3), activation='relu', padding='same', kernel_regularizer=l2(l2_lambda))(inputs)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling2D((2,2))(y)\n",
        "\n",
        "    y = Conv2D(64, (3,3), activation='relu', padding='same', kernel_regularizer=l2(l2_lambda))(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling2D((2,2))(y)\n",
        "\n",
        "    y = Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=l2(l2_lambda))(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = GlobalAveragePooling2D()(y)\n",
        "    y = Dropout(0.2)(y)\n",
        "    y = Dense(64, activation='relu', kernel_regularizer=l2(l2_lambda))(y)\n",
        "    y = Dropout(0.5)(y)\n",
        "    return inputs, y\n",
        "\n",
        "mfcc_in, mfcc_feat = mfcc_branch(X_train_st_aug.shape[1:])\n",
        "# Fix the shape for the Mel spectrogram input\n",
        "mel_in, mel_feat = mel_branch(X_mel_train_aug.shape[1:])  # Remove the last dimension in shape\n",
        "\n",
        "# Now you can safely concatenate the two branches\n",
        "merged = Concatenate()([mfcc_feat, mel_feat])\n",
        "x = Dense(128, activation='relu')(merged)\n",
        "x = Dropout(0.5)(x)\n",
        "out = Dense(num_emotions, activation='softmax')(x)\n",
        "model = Model([mfcc_in, mel_in], out)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 6ï¸âƒ£ Train\n",
        "# ----------------------------------------------------------------------\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "\n",
        "    [X_train_st_aug, X_mel_train_aug],\n",
        "    y_train_one_hot,\n",
        "    validation_data=([X_val_st, X_mel_val], y_val_one_hot),\n",
        "    batch_size=32,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 7ï¸âƒ£ Separate Testing by Gender\n",
        "# ----------------------------------------------------------------------\n",
        "gender_map = {spk:0 for spk in (train_speakers_male + val_speakers_male + test_speakers_male)}\n",
        "gender_map.update({spk:1 for spk in (train_speakers_female + val_speakers_female + test_speakers_female)})\n",
        "\n",
        "y_test_gender = np.array([gender_map[spk] for spk in speakers[test_idx]])\n",
        "y_true = np.argmax(y_test_one_hot, axis=1)\n",
        "y_pred_probs = model.predict([X_test_st, X_mel_test])\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "male_mask = (y_test_gender == 0)\n",
        "female_mask = (y_test_gender == 1)\n",
        "\n",
        "# ---- Male\n",
        "y_true_male = y_true[male_mask]\n",
        "y_pred_male = y_pred[male_mask]\n",
        "male_acc = np.mean(y_pred_male == y_true_male)\n",
        "male_precision = precision_score(y_true_male, y_pred_male, average='macro', zero_division=0)\n",
        "male_recall = recall_score(y_true_male, y_pred_male, average='macro', zero_division=0)\n",
        "male_f1 = f1_score(y_true_male, y_pred_male, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n--- ðŸ‘¨ Male Test Results ---\")\n",
        "print(f\"Accuracy : {male_acc*100:.2f}%\")\n",
        "print(f\"Precision: {male_precision*100:.2f}%\")\n",
        "print(f\"Recall   : {male_recall*100:.2f}%\")\n",
        "print(f\"F1-score : {male_f1*100:.2f}%\")\n",
        "print(classification_report(y_true_male, y_pred_male, target_names=emotion_encoder.classes_))\n",
        "cm_male = confusion_matrix(y_true_male, y_pred_male)\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm_male, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)\n",
        "plt.title(\"ðŸ‘¨ Confusion Matrix â€“ Male Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# ---- Female\n",
        "y_true_female = y_true[female_mask]\n",
        "y_pred_female = y_pred[female_mask]\n",
        "female_acc = np.mean(y_pred_female == y_true_female)\n",
        "female_precision = precision_score(y_true_female, y_pred_female, average='macro', zero_division=0)\n",
        "female_recall = recall_score(y_true_female, y_pred_female, average='macro', zero_division=0)\n",
        "female_f1 = f1_score(y_true_female, y_pred_female, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n--- ðŸ‘© Female Test Results ---\")\n",
        "print(f\"Accuracy : {female_acc*100:.2f}%\")\n",
        "print(f\"Precision: {female_precision*100:.2f}%\")\n",
        "print(f\"Recall   : {female_recall*100:.2f}%\")\n",
        "print(f\"F1-score : {female_f1*100:.2f}%\")\n",
        "print(classification_report(y_true_female, y_pred_female, target_names=emotion_encoder.classes_))\n",
        "cm_female = confusion_matrix(y_true_female, y_pred_female)\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm_female, annot=True, fmt='d', cmap='Reds',\n",
        "            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)\n",
        "plt.title(\"ðŸ‘© Confusion Matrix â€“ Female Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# ---- Difference\n",
        "diff_acc = male_acc - female_acc\n",
        "print(\"\\n--- ðŸ” Performance Difference (Male - Female) ---\")\n",
        "print(f\"Accuracy Difference : {diff_acc*100:.2f}%\")\n",
        "print(f\"Absolute Accuracy Gap : {abs(diff_acc)*100:.2f}%\")\n",
        "\n",
        "# ---- Overall\n",
        "overall_acc = np.mean(y_pred == y_true)\n",
        "overall_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "overall_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "overall_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n--- âš–ï¸ Overall Combined Results ---\")\n",
        "print(f\"Overall Accuracy : {overall_acc*100:.2f}%\")\n",
        "print(f\"Overall Precision: {overall_precision*100:.2f}%\")\n",
        "print(f\"Overall Recall   : {overall_recall*100:.2f}%\")\n",
        "print(f\"Overall F1-score : {overall_f1*100:.2f}%\")\n",
        "cm_overall = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_overall, annot=True, fmt='d', cmap='Purples',\n",
        "            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)\n",
        "plt.title(\"âš–ï¸ Confusion Matrix â€“ Combined Male + Female Test Set\")\n",
        "plt.show()\n",
        "# -------------------------------\n",
        "# 4ï¸âƒ£ Plot Accuracy and Loss Curves\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u3wxbVCOKd_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}