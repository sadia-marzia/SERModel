# -*- coding: utf-8 -*-
"""Neural1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nmpxQpxcetevoTYSdXYWV4T-4q_Dcy-P
"""

pip install tensorflow torch torchaudio librosa scikit-learn matplotlib seaborn pandas numpy

pip install tqdm ipywidgets

from google.colab import drive
drive.mount('/content/drive')

from tqdm.notebook import tqdm

import os

base_path = '/content/drive/MyDrive/ears data'
print("Contents of ears data folder:")
print(os.listdir(base_path))

import os

base_path = '/content/drive/MyDrive/ears data/p036'
print("Contents of ears data folder:")
print(os.listdir(base_path))

import os

# Example path (change if needed)
data_path = "/content/drive/MyDrive/ears data"

# Check participants
participants = sorted(os.listdir(data_path))
print("Participants:", participants[:10])  # show first 10

participant_path = os.path.join(data_path, 'p001')
print(os.listdir(participant_path))

import os

root_dir = "/content/drive/MyDrive/ears data/p001"

# List all .wav files in this folder
files = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith('.wav')]

print(f"Number of files in p001: {len(files)}")
print("First 5 files:", files[:5])

import librosa

for f in files[:5]:  # check first 5 files
    audio, sr = librosa.load(f, sr=None)  # sr=None preserves original sampling rate
    print(f"File: {os.path.basename(f)}, Sampling Rate: {sr} Hz, Duration: {len(audio)/sr:.2f}s")

import os

# Path to the root directory containing speaker folders
root_dir = "/content/drive/MyDrive/ears data"

# Create a dictionary to hold file paths for each speaker
speaker_files = {}

for speaker in os.listdir(root_dir):
    speaker_path = os.path.join(root_dir, speaker)
    if os.path.isdir(speaker_path):
        files = [os.path.join(speaker_path, f) for f in os.listdir(speaker_path) if f.endswith('.wav')]
        speaker_files[speaker] = files

# Example: Print first 5 files of speaker 'p001'
print("Files for p001:", speaker_files.get('p001', [])[:5])

import os
import librosa
import soundfile as sf
import numpy as np

# --- Parameters ---
root_dir = "/content/drive/MyDrive/ears data"  # Original dataset folder containing p001, p002, ...
output_dir = "/content/drive/MyDrive/ears_data_15s"  # Folder to save processed audio
target_sr = 16000
target_duration = 15  # seconds
target_length = target_sr * target_duration  # 15*16000 = 240000 samples

# Create output folder if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# --- Process each speaker folder ---
for speaker in os.listdir(root_dir):
    speaker_path = os.path.join(root_dir, speaker)
    if not os.path.isdir(speaker_path):
        continue

    # Create corresponding output folder for speaker
    out_speaker_path = os.path.join(output_dir, speaker)
    os.makedirs(out_speaker_path, exist_ok=True)

    # Process each audio file
    for file_name in os.listdir(speaker_path):
        if not file_name.endswith('.wav'):
            continue

        file_path = os.path.join(speaker_path, file_name)

        # Load audio and resample to 16kHz
        audio, sr = librosa.load(file_path, sr=target_sr)

        # Pad or truncate to 15 seconds
        if len(audio) < target_length:
            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')
        else:
            audio = audio[:target_length]

        # Save the processed audio
        out_file_path = os.path.join(out_speaker_path, file_name)
        sf.write(out_file_path, audio, target_sr)

    print(f"Processed all files for speaker {speaker}")

import os
import librosa
import numpy as np
import pandas as pd
from tqdm import tqdm

audio_dir = "/content/drive/MyDrive/ears_data_15s"
output_csv = "/content/drive/MyDrive/ears data/features_mfcc_segments.csv"

SAMPLE_RATE = 16000
PRE_EMPHASIS = 0.97
N_MFCC = 13
FRAME_SIZE = 469     # Number of frames per segment
FRAME_OVERLAP = 128  # Overlap between consecutive segments

def pre_emphasis(signal, alpha=0.97):
    return np.append(signal[0], signal[1:] - alpha * signal[:-1])

def segment_mfcc(mfcc, frame_size, overlap):
    step = frame_size - overlap
    segments = []
    for start in range(0, mfcc.shape[1] - frame_size + 1, step):
        seg = mfcc[:, start:start+frame_size]
        segments.append(seg)
    return np.array(segments)

features = []

for speaker in sorted(os.listdir(audio_dir)):
    speaker_path = os.path.join(audio_dir, speaker)
    if not os.path.isdir(speaker_path):
        continue

    print(f"Extracting segmented MFCCs for speaker: {speaker}")
    for file_name in tqdm(os.listdir(speaker_path)):
        if not file_name.endswith(".wav"):
            continue

        file_path = os.path.join(speaker_path, file_name)
        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)
        y = pre_emphasis(y, alpha=PRE_EMPHASIS)

        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)
        segments = segment_mfcc(mfcc, FRAME_SIZE, FRAME_OVERLAP)

        for seg_idx, seg in enumerate(segments):
            mfcc_mean = np.mean(seg, axis=1)
            mfcc_std = np.std(seg, axis=1)

            feature_dict = {"speaker": speaker, "file": file_name, "segment": seg_idx}
            for i in range(N_MFCC):
                feature_dict[f"mfcc_mean_{i+1}"] = mfcc_mean[i]
                feature_dict[f"mfcc_std_{i+1}"] = mfcc_std[i]
            features.append(feature_dict)

df = pd.DataFrame(features)
df.to_csv(output_csv, index=False)
print(f"âœ… Segmented MFCC extraction complete! Saved to: {output_csv}")

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

# === Paths ===
AUDIO_DIR = "/content/drive/MyDrive/ears_data_15s"
OUTPUT_MEL_DIR = "/content/drive/MyDrive/ears data/features_mel_spectrograms"
OUTPUT_CHROMA_DIR = "/content/drive/MyDrive/ears data/features_chroma"
OUTPUT_MEL_IMG_DIR = "/content/drive/MyDrive/ears data/features_mel_images"

# === Parameters ===
SAMPLE_RATE = 16000
N_MELS = 128
N_CHROMA = 12
N_FFT = 1024
HOP_LENGTH = 512

# === Create output folders ===
os.makedirs(OUTPUT_MEL_DIR, exist_ok=True)
os.makedirs(OUTPUT_CHROMA_DIR, exist_ok=True)
os.makedirs(OUTPUT_MEL_IMG_DIR, exist_ok=True)

# === Feature extraction loop ===
for speaker in sorted(os.listdir(AUDIO_DIR)):
    speaker_path = os.path.join(AUDIO_DIR, speaker)
    if not os.path.isdir(speaker_path):
        continue

    mel_save_dir = os.path.join(OUTPUT_MEL_DIR, speaker)
    chroma_save_dir = os.path.join(OUTPUT_CHROMA_DIR, speaker)
    mel_img_save_dir = os.path.join(OUTPUT_MEL_IMG_DIR, speaker)

    os.makedirs(mel_save_dir, exist_ok=True)
    os.makedirs(chroma_save_dir, exist_ok=True)
    os.makedirs(mel_img_save_dir, exist_ok=True)

    print(f"Extracting features for speaker: {speaker}")

    for file_name in tqdm(os.listdir(speaker_path)):
        if not file_name.endswith(".wav"):
            continue

        file_path = os.path.join(speaker_path, file_name)

        # === Load audio ===
        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)

        # === 1. Mel-Spectrogram ===
        mel_spec = librosa.feature.melspectrogram(
            y=y, sr=sr, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=2.0
        )
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

        # === Save as .npy ===
        mel_save_path = os.path.join(mel_save_dir, file_name.replace(".wav", "_mel.npy"))
        np.save(mel_save_path, mel_spec_db)

        # === Save as PNG Image ===
        mel_img_path = os.path.join(mel_img_save_dir, file_name.replace(".wav", "_mel.png"))
        plt.figure(figsize=(4, 4))  # small figure
        librosa.display.specshow(
            mel_spec_db,
            sr=sr,
            hop_length=HOP_LENGTH,
            x_axis='time',
            y_axis='mel',
            cmap='magma'
        )
        plt.axis('off')  # remove axes for cleaner image
        plt.tight_layout()
        plt.savefig(mel_img_path, bbox_inches='tight', pad_inches=0)
        plt.close()

        # === 2. Chroma features ===
        chroma = librosa.feature.chroma_stft(
            y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_chroma=N_CHROMA
        )
        chroma_save_path = os.path.join(chroma_save_dir, file_name.replace(".wav", "_chroma.npy"))
        np.save(chroma_save_path, chroma)

print("âœ… Mel-spectrogram PNGs, Mel and Chroma features extraction complete!")
print(f"ðŸ“ Mel images saved to: {OUTPUT_MEL_IMG_DIR}")

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import librosa.display
from tqdm import tqdm

# === Set the speaker folder you want to process ===
SPEAKER_ID = "p098"  # ðŸ”¹ change this for each run (e.g., p002, p003, ...)

# === Paths ===
AUDIO_DIR = f"/content/drive/MyDrive/ears_data_15s/{SPEAKER_ID}"
OUTPUT_MEL_DIR = f"/content/drive/MyDrive/ears_data/features_mel_spectrograms/{SPEAKER_ID}"
OUTPUT_CHROMA_DIR = f"/content/drive/MyDrive/ears_data/features_chroma/{SPEAKER_ID}"
OUTPUT_MEL_IMG_DIR = f"/content/drive/MyDrive/ears_data/features_mel_images/{SPEAKER_ID}"

# === Parameters ===
SAMPLE_RATE = 16000
N_MELS = 128
N_CHROMA = 12
N_FFT = 1024
HOP_LENGTH = 512

# === Create output folders ===
os.makedirs(OUTPUT_MEL_DIR, exist_ok=True)
os.makedirs(OUTPUT_CHROMA_DIR, exist_ok=True)
os.makedirs(OUTPUT_MEL_IMG_DIR, exist_ok=True)

print(f"ðŸŽ§ Extracting features for: {SPEAKER_ID}")

# === Feature extraction loop ===
for file_name in tqdm(os.listdir(AUDIO_DIR)):
    if not file_name.endswith(".wav"):
        continue

    file_path = os.path.join(AUDIO_DIR, file_name)

    # === Load audio ===
    y, sr = librosa.load(file_path, sr=SAMPLE_RATE)

    # === 1. Mel-Spectrogram ===
    mel_spec = librosa.feature.melspectrogram(
        y=y, sr=sr, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=2.0
    )
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

    # === Save as .npy ===
    mel_save_path = os.path.join(OUTPUT_MEL_DIR, file_name.replace(".wav", "_mel.npy"))
    np.save(mel_save_path, mel_spec_db)

    # === Save as PNG Image ===
    mel_img_path = os.path.join(OUTPUT_MEL_IMG_DIR, file_name.replace(".wav", "_mel.png"))
    plt.figure(figsize=(4, 4))
    librosa.display.specshow(
        mel_spec_db,
        sr=sr,
        hop_length=HOP_LENGTH,
        x_axis='time',
        y_axis='mel',
        cmap='magma'
    )
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(mel_img_path, bbox_inches='tight', pad_inches=0)
    plt.close()

    # === 2. Chroma features ===
    chroma = librosa.feature.chroma_stft(
        y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_chroma=N_CHROMA
    )
    chroma_save_path = os.path.join(OUTPUT_CHROMA_DIR, file_name.replace(".wav", "_chroma.npy"))
    np.save(chroma_save_path, chroma)

print(f"âœ… Completed feature extraction for {SPEAKER_ID}")
print(f"ðŸ“ Mel images saved to: {OUTPUT_MEL_IMG_DIR}")

import os

BASE_DIR = "/content/drive/MyDrive/ears_data/features_mel_images"

total = 0
for speaker in os.listdir(BASE_DIR):
    speaker_path = os.path.join(BASE_DIR, speaker)
    if os.path.isdir(speaker_path):
        count = len([f for f in os.listdir(speaker_path) if f.endswith(".png")])
        print(f"{speaker}: {count}")
        total += count

print(f"\nTotal mel-spectrogram images across all speakers: {total}")

import os
import shutil
from tqdm import tqdm

# === Emotion list (target emotions) ===
emotions = [
    "adoration", "anger", "amazement", "amusement", "confusion", "contentment",
    "cuteness", "desire", "disappointment", "disgust", "distress", "embarassment",
    "extasy", "fear", "guilt", "interest", "neutral", "sadness", "pain",
    "pride", "realization", "relief", "serenity"
]

# === Source and Destination Paths ===
SOURCE_ROOT = "/content/drive/MyDrive/ears_data/features_mel_images"
DEST_ROOT = "/content/drive/MyDrive/ears_data/features_mel_image23"

# === Create destination folder ===
os.makedirs(DEST_ROOT, exist_ok=True)

# === Speaker-wise Copy ===
speakers = sorted([d for d in os.listdir(SOURCE_ROOT) if os.path.isdir(os.path.join(SOURCE_ROOT, d))])

summary = {}

for spk in tqdm(speakers, desc="Copying speaker-wise images"):
    src_spk_dir = os.path.join(SOURCE_ROOT, spk)
    dst_spk_dir = os.path.join(DEST_ROOT, spk)
    os.makedirs(dst_spk_dir, exist_ok=True)

    count = 0
    for file_name in os.listdir(src_spk_dir):
        if file_name.endswith(".png") and file_name.startswith("emo_"):
            # Check if the emotion name exists in the list
            if any(f"emo_{emo}_" in file_name for emo in emotions):
                shutil.copy2(os.path.join(src_spk_dir, file_name),
                             os.path.join(dst_spk_dir, file_name))
                count += 1

    summary[spk] = count

print("\nâœ… All speaker folders copied successfully to:", DEST_ROOT)
print("\nðŸ“Š Summary (Number of images per speaker):")
for spk, cnt in summary.items():
    print(f"{spk}: {cnt} images")

import os

BASE_DIR = "/content/drive/MyDrive/ears_data/features_mel_image23"

total = 0
for speaker in os.listdir(BASE_DIR):
    speaker_path = os.path.join(BASE_DIR, speaker)
    if os.path.isdir(speaker_path):
        count = len([f for f in os.listdir(speaker_path) if f.endswith(".png")])
        print(f"{speaker}: {count}")
        total += count

print(f"\nTotal mel-spectrogram images across all speakers: {total}")

import os
import shutil
from tqdm import tqdm

# === Emotion list (target emotions) ===
emotions = [
    "adoration", "anger", "amazement", "amusement", "confusion", "contentment",
    "cuteness", "desire", "disappointment", "disgust", "distress", "embarassment",
    "extasy", "fear", "guilt", "interest", "neutral", "sadness", "pain",
    "pride", "realization", "relief", "serenity"
]

# === Source and Destination Paths ===
SOURCE_ROOT = "/content/drive/MyDrive/ears_data/features_chroma"
DEST_ROOT = "/content/drive/MyDrive/ears_data/features_chroma23"

# === Create destination folder ===
os.makedirs(DEST_ROOT, exist_ok=True)

# === Speaker-wise Copy ===
speakers = sorted([d for d in os.listdir(SOURCE_ROOT) if os.path.isdir(os.path.join(SOURCE_ROOT, d))])

summary = {}

for spk in tqdm(speakers, desc="Copying speaker-wise images"):
    src_spk_dir = os.path.join(SOURCE_ROOT, spk)
    dst_spk_dir = os.path.join(DEST_ROOT, spk)
    os.makedirs(dst_spk_dir, exist_ok=True)

    count = 0
    for file_name in os.listdir(src_spk_dir):
        if file_name.endswith(".npy") and file_name.startswith("emo_"):
            # Check if the emotion name exists in the list
            if any(f"emo_{emo}_" in file_name for emo in emotions):
                shutil.copy2(os.path.join(src_spk_dir, file_name),
                             os.path.join(dst_spk_dir, file_name))
                count += 1

    summary[spk] = count

print("\nâœ… All speaker folders copied successfully to:", DEST_ROOT)
print("\nðŸ“Š Summary (Number of chroma per speaker):")
for spk, cnt in summary.items():
    print(f"{spk}: {cnt} files")

import os

BASE_DIR = "/content/drive/MyDrive/ears_data/features_chroma23"

total = 0
for speaker in os.listdir(BASE_DIR):
    speaker_path = os.path.join(BASE_DIR, speaker)
    if os.path.isdir(speaker_path):
        count = len([f for f in os.listdir(speaker_path) if f.endswith(".npy")])
        print(f"{speaker}: {count}")
        total += count

print(f"\nTotal chroma across all speakers: {total}")

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Load audio
file_path = "/content/drive/MyDrive/ears data/p001/emo_amusement_sentences_aug_pitch_down.wav"
y, sr = librosa.load(file_path, sr=16000)

# Compute STFT
n_fft = 16384  # large FFT for high frequency resolution
hop_length = 512
D = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))

# Convert to dB
DB = librosa.amplitude_to_db(D, ref=np.max)

plt.figure(figsize=(12, 6))
librosa.display.specshow(DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='hz', cmap='magma')
plt.colorbar(format='%+2.0f dB')
plt.title("Spectrogram (Frequency in Hz vs Time)")
plt.xlabel("Time (s)")
plt.ylabel("Frequency (Hz)")
plt.tight_layout()
plt.show()

import os
import numpy as np
import librosa
import matplotlib.pyplot as plt

# --- Parameters ---
SAMPLE_RATE = 16000
N_MFCC = 13
PRE_EMPHASIS = 0.97

# --- Helper: Pre-emphasis ---
def pre_emphasis(signal, alpha=0.97):
    return np.append(signal[0], signal[1:] - alpha * signal[:-1])

# --- Choose audio file (replace with your path) ---
audio_file = "/content/drive/MyDrive/ears_data_15s/p001/emo_adoration_freeform_orig.wav"

# --- Load audio ---
y, sr = librosa.load(audio_file, sr=SAMPLE_RATE)
y = pre_emphasis(y, PRE_EMPHASIS)

# --- Compute MFCC ---
mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)

# --- Plot MFCC heatmap ---
plt.figure(figsize=(10, 4))
plt.imshow(mfcc, origin='lower', aspect='auto', cmap='magma')
plt.xlabel("Time frames")
plt.ylabel("MFCC coefficients")
plt.title("MFCC Features (Fig.1 style)")
plt.colorbar(label="Amplitude")
plt.tight_layout()
plt.show()

import os

# Path to the root directory containing speaker folders
root_dir = "/content/drive/MyDrive/ears data/p001"

# Create a dictionary to hold file paths for each speaker
speaker_files = {}

for speaker in os.listdir(root_dir):
    speaker_path = os.path.join(root_dir, speaker)
    if os.path.isdir(speaker_path):
        files = [os.path.join(speaker_path, f) for f in os.listdir(speaker_path) if f.endswith('.wav')]
        speaker_files[speaker] = files

# Example: Print first 5 files of speaker 'p001'
print("Files for p001:", speaker_files.get('p001', [])[:5])

import librosa

for speaker, files in speaker_files.items():
    print(f"\nSpeaker: {speaker}")
    for file_path in files[:5]:  # check first 5 files for brevity
        y, sr = librosa.load(file_path, sr=None)  # sr=None preserves original sample rate
        print(f"{os.path.basename(file_path)} - Sampling rate: {sr} Hz")

train_male = ['p007','p008','p010','p017','p023','p040','p046','p065','p071','p076','p086','p090','p094','p095','p101']
train_female = ['p011','p013','p018','p021','p041','p043','p048','p055','p056','p062','p075','p083','p098','p104','p106']

val_male = ['p019','p025','p038','p039','p102']
val_female = ['p027','p028','p044','p053','p079']

test_male = ['p001','p074','p081','p085','p105']
test_female = ['p012','p029','p061','p068','p070']

train_speakers = train_male + train_female
val_speakers = val_male + val_female
test_speakers = test_male + test_female

import os
import json
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Dense, Dropout, LayerNormalization,
                                     MultiHeadAttention, Flatten, Conv2D,
                                     BatchNormalization, MaxPooling2D,
                                     GlobalAveragePooling2D, Lambda, Add)
from tensorflow.keras.callbacks import EarlyStopping

# --- 1. Load All Pre-processed Data and Real Gender Info ---
EMOTION_DATA_DIR = '/content/drive/MyDrive/dataset/emotion_features_specialist'
MULTITASK_DATA_DIR = '/content/drive/MyDrive/dataset/curated_multitask_features'
GENDER_JSON_PATH = 'speaker_statistics.json'

print("Loading pre-processed data and gender information...")
X = np.load(os.path.join(EMOTION_DATA_DIR, 'X_emotion.npy'))
y_emotions = np.load(os.path.join(EMOTION_DATA_DIR, 'y_emotions.npy'))
y_speakers = np.load(os.path.join(MULTITASK_DATA_DIR, 'y_speakers_curated.npy'))

with open(GENDER_JSON_PATH, 'r') as f:
    speaker_stats = json.load(f)

gender_map = {
    speaker_id: 0 if stats.get('gender') == 'male' else 1
    for speaker_id, stats in speaker_stats.items()
    if stats.get('gender') in ['male', 'female']
}
valid_speakers = list(gender_map.keys())
valid_mask = np.isin(y_speakers, valid_speakers)
X, y_emotions, y_speakers = X[valid_mask], y_emotions[valid_mask], y_speakers[valid_mask]
y_gender = np.array([gender_map[spk_id] for spk_id in y_speakers])

# --- 2. Balanced and Stratified Speaker-Independent Split ---
print("\nPerforming BALANCED and STRATIFIED speaker-independent split...")
all_male_speakers = [spk for spk in np.unique(y_speakers) if gender_map[spk] == 0]
all_female_speakers = [spk for spk in np.unique(y_speakers) if gender_map[spk] == 1]
np.random.seed(42); np.random.shuffle(all_male_speakers); np.random.shuffle(all_female_speakers)

selected_male_speakers = all_male_speakers[:25]
selected_female_speakers = all_female_speakers[:25]

train_male, val_male, test_male = selected_male_speakers[:15], selected_male_speakers[15:20], selected_male_speakers[20:]
train_female, val_female, test_female = selected_female_speakers[:15], selected_female_speakers[15:20], selected_female_speakers[20:]

train_speaker_ids = np.array(train_male + train_female)
val_speaker_ids = np.array(val_male + val_female)
test_speaker_ids = np.array(test_male + test_female)

train_mask = np.isin(y_speakers, train_speaker_ids)
X_train, y_train_emo = X[train_mask], y_emotions[train_mask]
val_mask = np.isin(y_speakers, val_speaker_ids)
X_val, y_val_emo = X[val_mask], y_emotions[val_mask]
test_mask = np.isin(y_speakers, test_speaker_ids)
X_test, y_test_emo, y_test_gender = X[test_mask], y_emotions[test_mask], y_gender[test_mask]

# --- 3. Standardize and Prepare Data ---
scaler = StandardScaler()
X_train_3d = np.squeeze(X_train); scaler.fit(X_train_3d.reshape(-1, X_train_3d.shape[-1]))
X_train_scaled = scaler.transform(X_train_3d.reshape(-1, X_train_3d.shape[-1])).reshape(X_train_3d.shape)
X_val_scaled = scaler.transform(np.squeeze(X_val).reshape(-1, X_val.shape[-1])).reshape(np.squeeze(X_val).shape)
X_test_scaled = scaler.transform(np.squeeze(X_test).reshape(-1, X_test.shape[-1])).reshape(np.squeeze(X_test).shape)
X_train, X_val, X_test = X_train_scaled[..., np.newaxis], X_val_scaled[..., np.newaxis], X_test_scaled[..., np.newaxis]

emotion_encoder = LabelEncoder(); emotion_encoder.fit(np.concatenate([y_train_emo, y_val_emo, y_test_emo]))
num_emotions = len(emotion_encoder.classes_)
y_train_one_hot = to_categorical(emotion_encoder.transform(y_train_emo), num_classes=num_emotions)
y_val_one_hot = to_categorical(emotion_encoder.transform(y_val_emo), num_classes=num_emotions)
y_test_one_hot = to_categorical(emotion_encoder.transform(y_test_emo), num_classes=num_emotions)

# --- 4. Define "Deeper and Wider" Model ---
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = LayerNormalization(epsilon=1e-6)(inputs)
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = Dropout(dropout)(x)
    res = Add()([x, inputs])
    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation="relu")(x)
    x = Dropout(dropout)(x)
    x = Dense(inputs.shape[-1])(x)
    return Add()([x, res])

def create_emotion_model(input_shape, num_emotions):
    inputs = Input(shape=input_shape)
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x); x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x); x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x); x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    x = Lambda(lambda t: tf.expand_dims(t, axis=1), output_shape=(1, 128))(x)
    shared_output = transformer_encoder(x, head_size=128, num_heads=4, ff_dim=128, dropout=0.3)
    shared_output = Flatten()(shared_output)
    emotion_head = Dense(128, activation='relu')(shared_output)
    emotion_head = Dropout(0.5)(emotion_head)
    emotion_output = Dense(num_emotions, activation='softmax')(emotion_head)
    model = Model(inputs=inputs, outputs=emotion_output)
    return model

# --- 5. Train BASELINE Model ---
print("\n" + "="*50 + "\nTRAINING BASELINE MODEL (NO DEBIASING)\n" + "="*50)
baseline_model = create_emotion_model(X_train.shape[1:], num_emotions)
baseline_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
baseline_model.summary()

history = baseline_model.fit(
    X_train, y_train_one_hot, validation_data=(X_val, y_val_one_hot),
    epochs=50, batch_size=64, verbose=1,
    callbacks=[EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True)]
)

# --- 6. Final Fairness Evaluation ---
print("\n" + "="*50 + "\nFINAL FAIRNESS EVALUATION ON TEST SET\n" + "="*50)
male_mask = y_test_gender == 0
female_mask = y_test_gender == 1
X_test_male, y_test_male = X_test[male_mask], y_test_one_hot[male_mask]
X_test_female, y_test_female = X_test[female_mask], y_test_one_hot[female_mask]

print("\n--- Baseline Model Performance ---")
_, acc_male_baseline = baseline_model.evaluate(X_test_male, y_test_male, verbose=0)
_, acc_female_baseline = baseline_model.evaluate(X_test_female, y_test_female, verbose=0)
print(f"Accuracy on Male Speakers: {acc_male_baseline*100:.2f}%")
print(f"Accuracy on Female Speakers: {acc_female_baseline*100:.2f}%")
print(f"PERFORMANCE GAP: {abs(acc_male_baseline - acc_female_baseline)*100:.2f}%")

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, MaxPooling2D,
                                     GlobalAveragePooling2D, Dense, Dropout, Flatten, Lambda,
                                     LayerNormalization, MultiHeadAttention, Add)
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ----------------------------------------------------------------------
# 1ï¸âƒ£ Transformer Encoder Block
# ----------------------------------------------------------------------
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = LayerNormalization(epsilon=1e-6)(inputs)
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = Dropout(dropout)(x)
    res = Add()([x, inputs])

    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation="relu")(x)
    x = Dropout(dropout)(x)
    x = Dense(inputs.shape[-1])(x)
    return Add()([x, res])

# ----------------------------------------------------------------------
# 2ï¸âƒ£ Model Definition
# ----------------------------------------------------------------------
def create_emotion_model(input_shape, num_emotions):
    inputs = Input(shape=input_shape)
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x); x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x); x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x); x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    x = Lambda(lambda t: tf.expand_dims(t, axis=1), output_shape=(1, 128))(x)

    shared_output = transformer_encoder(x, head_size=128, num_heads=4, ff_dim=128, dropout=0.3)
    shared_output = Flatten()(shared_output)
    emotion_head = Dense(128, activation='relu')(shared_output)
    emotion_head = Dropout(0.5)(emotion_head)
    emotion_output = Dense(num_emotions, activation='softmax')(emotion_head)
    model = Model(inputs=inputs, outputs=emotion_output)
    return model

# ----------------------------------------------------------------------
# 3ï¸âƒ£ Load and Prepare Data
# ----------------------------------------------------------------------
npy_dir = "/content/drive/MyDrive/ears_data/mfcc_segments_23emotions"
emotions = ["adoration", "anger", "fear", "neutral", "sadness"]

segments, labels, speakers = [], [], []
all_files = [f for f in os.listdir(npy_dir) if f.endswith('.npy')]

selected_emotions = ["adoration", "anger", "fear", "neutral", "sadness"]

segments_filtered = []
labels_filtered = []
speakers_filtered = []

for f in all_files:
    if "emo_" in f:
        emotion = f.split("emo_")[1].split("_")[0]
        if emotion not in selected_emotions:
            continue  # skip any unwanted emotions
    else:
        continue

    seg_data = np.load(os.path.join(npy_dir, f))
    speaker = f.split("_")[0]

    segments_filtered.append(seg_data)
    labels_filtered.append(emotion)
    speakers_filtered.append(speaker)

X = np.array(segments_filtered)
y = np.array(labels_filtered)
speakers = np.array(speakers_filtered)

X = X[..., np.newaxis]
y_encoded = LabelEncoder().fit_transform(y)
y_onehot = to_categorical(y_encoded, num_classes=len(selected_emotions))

# ----------------------------------------------------------------------
# 4ï¸âƒ£ Speaker-based Split
# ----------------------------------------------------------------------
train_speakers_male = ['p007','p008','p010','p017','p023','p040','p046','p065','p071','p076','p086','p090','p094','p095','p101']
train_speakers_female = ['p011','p013','p018','p021','p041','p043','p048','p055','p056','p062','p075','p083','p098','p104','p106']
val_speakers_male = ['p019','p025','p038','p039','p102']
val_speakers_female = ['p027','p028','p044','p053','p079']
test_speakers_male = ['p001','p074','p081','p085','p105']
test_speakers_female = ['p012','p029','p061','p068','p070']

train_speakers = train_speakers_male + train_speakers_female
val_speakers = val_speakers_male + val_speakers_female
test_speakers = test_speakers_male + test_speakers_female

train_idx = np.isin(speakers, train_speakers)
val_idx = np.isin(speakers, val_speakers)
test_idx = np.isin(speakers, test_speakers)

X_train, y_train_one_hot = X[train_idx], y_onehot[train_idx]
X_val, y_val_one_hot = X[val_idx], y_onehot[val_idx]
X_test, y_test_one_hot = X[test_idx], y_onehot[test_idx]
y_test = y_encoded[test_idx]

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")

# ----------------------------------------------------------------------
# 5ï¸âƒ£ Train Model
# ----------------------------------------------------------------------
model = create_emotion_model(X_train.shape[1:], num_emotions=len(emotions))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

history = model.fit(
    X_train, y_train_one_hot,
    validation_data=(X_val, y_val_one_hot),
    epochs=50,
    batch_size=64,
    verbose=1,
    callbacks=[EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True)]
)

# ----------------------------------------------------------------------
# 6ï¸âƒ£ Evaluate on Test Set with Metrics
# ----------------------------------------------------------------------
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

test_loss, test_acc = model.evaluate(X_test, y_test_one_hot, verbose=1)
print(f"\nâœ… Test Accuracy: {test_acc:.4f}")

# --- Classification Report ---
print("\nðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred, target_names=emotions, digits=4))

# --- Confusion Matrix ---
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=emotions, yticklabels=emotions)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, MaxPooling2D,
                                     GlobalAveragePooling2D, Dense, Dropout, Flatten, Lambda,
                                     LayerNormalization, MultiHeadAttention, Add)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ----------------------------------------------------------------------
# 1ï¸âƒ£ Transformer Encoder Block
# ----------------------------------------------------------------------
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = LayerNormalization(epsilon=1e-6)(inputs)
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = Dropout(dropout)(x)
    res = Add()([x, inputs])

    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation="relu")(x)
    x = Dropout(dropout)(x)
    x = Dense(inputs.shape[-1])(x)
    return Add()([x, res])

# ----------------------------------------------------------------------
# 2ï¸âƒ£ Model Definition
# ----------------------------------------------------------------------
def create_emotion_model(input_shape, num_emotions):
    inputs = Input(shape=input_shape)
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x); x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x); x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x); x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    x = Lambda(lambda t: tf.expand_dims(t, axis=1), output_shape=(1, 128))(x)

    shared_output = transformer_encoder(x, head_size=128, num_heads=4, ff_dim=128, dropout=0.3)
    shared_output = Flatten()(shared_output)
    emotion_head = Dense(128, activation='relu')(shared_output)
    emotion_head = Dropout(0.5)(emotion_head)
    emotion_output = Dense(num_emotions, activation='softmax')(emotion_head)
    model = Model(inputs=inputs, outputs=emotion_output)
    return model

# ----------------------------------------------------------------------
# 3ï¸âƒ£ Load and Prepare Data
# ----------------------------------------------------------------------
npy_dir = "/content/drive/MyDrive/ears_data/mfcc_segments_23emotions"
emotions = ["adoration", "anger", "fear", "neutral", "sadness"]

segments, labels, speakers = [], [], []
all_files = [f for f in os.listdir(npy_dir) if f.endswith('.npy')]

selected_emotions = ["adoration", "anger", "fear", "neutral", "sadness"]

segments_filtered = []
labels_filtered = []
speakers_filtered = []

for f in all_files:
    if "emo_" in f:
        emotion = f.split("emo_")[1].split("_")[0]
        if emotion not in selected_emotions:
            continue  # skip any unwanted emotions
    else:
        continue

    seg_data = np.load(os.path.join(npy_dir, f))
    speaker = f.split("_")[0]

    segments_filtered.append(seg_data)
    labels_filtered.append(emotion)
    speakers_filtered.append(speaker)

X = np.array(segments_filtered)
y = np.array(labels_filtered)
speakers = np.array(speakers_filtered)

X = X[..., np.newaxis]
y_encoded = LabelEncoder().fit_transform(y)
y_onehot = to_categorical(y_encoded, num_classes=len(selected_emotions))

# ----------------------------------------------------------------------
# 4ï¸âƒ£ Speaker-based Split
# ----------------------------------------------------------------------
train_speakers_male = ['p007','p008','p010','p017','p023','p040','p046','p065','p071','p076','p086','p090','p094','p095','p101']
train_speakers_female = ['p011','p013','p018','p021','p041','p043','p048','p055','p056','p062','p075','p083','p098','p104','p106']
val_speakers_male = ['p019','p025','p038','p039','p102']
val_speakers_female = ['p027','p028','p044','p053','p079']
test_speakers_male = ['p001','p074','p081','p085','p105']
test_speakers_female = ['p012','p029','p061','p068','p070']

train_speakers = train_speakers_male + train_speakers_female
val_speakers = val_speakers_male + val_speakers_female
test_speakers = test_speakers_male + test_speakers_female

train_idx = np.isin(speakers, train_speakers)
val_idx = np.isin(speakers, val_speakers)
test_idx = np.isin(speakers, test_speakers)

X_train, y_train_one_hot = X[train_idx], y_onehot[train_idx]
X_val, y_val_one_hot = X[val_idx], y_onehot[val_idx]
X_test, y_test_one_hot = X[test_idx], y_onehot[test_idx]
y_test = y_encoded[test_idx]

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")

# ----------------------------------------------------------------------
# 5ï¸âƒ£ Train Model
# ----------------------------------------------------------------------
model = create_emotion_model(X_train.shape[1:], num_emotions=len(selected_emotions))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Callbacks
early_stop = EarlyStopping(
    monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,      # reduce LR by half
    patience=5,      # wait 5 epochs before reducing
    min_lr=1e-7,     # lower bound
    verbose=1
)

history = model.fit(
    X_train, y_train_one_hot,
    validation_data=(X_val, y_val_one_hot),
    epochs=100,
    batch_size=64,
    verbose=1,
    callbacks=[early_stop, reduce_lr]  # added reduce_lr here
)

# ----------------------------------------------------------------------
# 6ï¸âƒ£ Evaluate on Test Set with Metrics
# ----------------------------------------------------------------------
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

test_loss, test_acc = model.evaluate(X_test, y_test_one_hot, verbose=1)
print(f"\nâœ… Test Accuracy: {test_acc:.4f}")

# --- Classification Report ---
print("\nðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred, target_names=emotions, digits=4))

# --- Confusion Matrix ---
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=emotions, yticklabels=emotions)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

"""# **whatsapp given code**"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, MaxPooling2D,
                                     GlobalAveragePooling2D, Dense, Dropout, Flatten, Lambda,
                                     LayerNormalization, MultiHeadAttention, Add)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ----------------------------------------------------------------------
# 1ï¸âƒ£ Transformer Encoder Block
# ----------------------------------------------------------------------
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = LayerNormalization(epsilon=1e-6)(inputs)
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = Dropout(dropout)(x)
    res = Add()([x, inputs])

    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation="relu")(x)
    x = Dropout(dropout)(x)
    x = Dense(inputs.shape[-1])(x)
    return Add()([x, res])

# ----------------------------------------------------------------------
# 2ï¸âƒ£ Model Definition
# ----------------------------------------------------------------------
def create_emotion_model(input_shape, num_emotions):
    inputs = Input(shape=input_shape)
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x); x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x); x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x); x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    x = Lambda(lambda t: tf.expand_dims(t, axis=1), output_shape=(1, 128))(x)

    shared_output = transformer_encoder(x, head_size=128, num_heads=4, ff_dim=128, dropout=0.3)
    shared_output = Flatten()(shared_output)
    emotion_head = Dense(128, activation='relu')(shared_output)
    emotion_head = Dropout(0.5)(emotion_head)
    emotion_output = Dense(num_emotions, activation='softmax')(emotion_head)
    model = Model(inputs=inputs, outputs=emotion_output)
    return model

# ----------------------------------------------------------------------
# 3ï¸âƒ£ Load and Filter Data
# ----------------------------------------------------------------------
npy_dir = "/content/drive/MyDrive/ears_data/mfcc_segments_23emotions"
selected_emotions = ["adoration", "anger", "fear", "neutral", "sadness"]

segments, labels, speakers = [], [], []
all_files = [f for f in os.listdir(npy_dir) if f.endswith('.npy')]

for f in all_files:
    if "emo_" in f:
        emotion = f.split("emo_")[1].split("_")[0]
        if emotion not in selected_emotions:
            continue
    else:
        continue
    seg_data = np.load(os.path.join(npy_dir, f))
    speaker = f.split("_")[0]
    segments.append(seg_data)
    labels.append(emotion)
    speakers.append(speaker)

X = np.array(segments)[..., np.newaxis]
y_emotions = np.array(labels)
y_speakers = np.array(speakers)

# ----------------------------------------------------------------------
# 4ï¸âƒ£ Define Speaker Splits & Gender Map
# ----------------------------------------------------------------------
train_speakers_male = ['p007','p008','p010','p017','p023','p040','p046','p065','p071','p076','p086','p090','p094','p095','p101']
train_speakers_female = ['p002','p006','p024','p026','p041','p043','p047','p055','p063','p067','p080','p083','p084','p104','p098']
val_speakers_male = ['p019','p025','p038','p039','p102']
val_speakers_female = ['p027','p034','p037','p052','p079']
test_speakers_male = ['p001','p074','p081','p085','p105']
test_speakers_female = ['p012','p036','p061','p068','p073']

# Combine train/val/test speakers
train_speakers = train_speakers_male + train_speakers_female
val_speakers = val_speakers_male + val_speakers_female
test_speakers = test_speakers_male + test_speakers_female

# gender map: 0=male, 1=female
gender_map = {spk:0 for spk in train_speakers_male + val_speakers_male + test_speakers_male}
gender_map.update({spk:1 for spk in train_speakers_female + val_speakers_female + test_speakers_female})
y_gender = np.array([gender_map[spk] for spk in y_speakers if spk in gender_map])

# Remove speakers not in split
valid_mask = np.isin(y_speakers, list(gender_map.keys()))
X, y_emotions, y_speakers = X[valid_mask], y_emotions[valid_mask], y_speakers[valid_mask]
y_gender = np.array([gender_map[spk] for spk in y_speakers])

# Split indices
train_ids = np.array(train_speakers_male + train_speakers_female + val_speakers_male + val_speakers_female + test_speakers_male + test_speakers_female)
train_mask = np.isin(y_speakers, train_speakers_male + train_speakers_female)
val_mask = np.isin(y_speakers, val_speakers_male + val_speakers_female)
test_mask = np.isin(y_speakers, test_speakers_male + test_speakers_female)

X_train, y_train_emo = X[train_mask], y_emotions[train_mask]
X_val, y_val_emo = X[val_mask], y_emotions[val_mask]
X_test, y_test_emo = X[test_mask], y_emotions[test_mask]
y_test_gender = y_gender[test_mask]

# ----------------------------------------------------------------------
# 5ï¸âƒ£ Standardize Data
# ----------------------------------------------------------------------
X_train_3d = np.squeeze(X_train)
X_val_3d   = np.squeeze(X_val)
X_test_3d  = np.squeeze(X_test)

X_train_flat = X_train_3d.reshape(len(X_train_3d), -1)
X_val_flat   = X_val_3d.reshape(len(X_val_3d), -1)
X_test_flat  = X_test_3d.reshape(len(X_test_3d), -1)

scaler = StandardScaler()
scaler.fit(X_train_flat)

X_train_scaled = scaler.transform(X_train_flat).reshape(X_train_3d.shape)[..., np.newaxis]
X_val_scaled   = scaler.transform(X_val_flat).reshape(X_val_3d.shape)[..., np.newaxis]
X_test_scaled  = scaler.transform(X_test_flat).reshape(X_test_3d.shape)[..., np.newaxis]

X_train, X_val, X_test = X_train_scaled, X_val_scaled, X_test_scaled

# Encode emotions
emotion_encoder = LabelEncoder()
emotion_encoder.fit(np.concatenate([y_train_emo, y_val_emo, y_test_emo]))
num_emotions = len(emotion_encoder.classes_)
y_train_one_hot = to_categorical(emotion_encoder.transform(y_train_emo), num_classes=num_emotions)
y_val_one_hot   = to_categorical(emotion_encoder.transform(y_val_emo), num_classes=num_emotions)
y_test_one_hot  = to_categorical(emotion_encoder.transform(y_test_emo), num_classes=num_emotions)

# ----------------------------------------------------------------------
# 6ï¸âƒ£ Train Model with ReduceLROnPlateau
# ----------------------------------------------------------------------
baseline_model = create_emotion_model(X_train.shape[1:], num_emotions)
baseline_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
baseline_model.summary()

reduce_lr = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

history = baseline_model.fit(
    X_train, y_train_one_hot,
    validation_data=(X_val, y_val_one_hot),
    epochs=50,
    batch_size=64,
    verbose=1,
    callbacks=[EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True),
               reduce_lr]
)

# ----------------------------------------------------------------------
# 7ï¸âƒ£ Evaluate Gender-wise Performance
# ----------------------------------------------------------------------
male_mask = y_test_gender == 0
female_mask = y_test_gender == 1
X_test_male, y_test_male = X_test[male_mask], y_test_one_hot[male_mask]
X_test_female, y_test_female = X_test[female_mask], y_test_one_hot[female_mask]

print("\n--- Baseline Model Performance ---")
_, acc_male_baseline = baseline_model.evaluate(X_test_male, y_test_male, verbose=0)
_, acc_female_baseline = baseline_model.evaluate(X_test_female, y_test_female, verbose=0)
print(f"Accuracy on Male Speakers: {acc_male_baseline*100:.2f}%")
print(f"Accuracy on Female Speakers: {acc_female_baseline*100:.2f}%")
print(f"PERFORMANCE GAP: {abs(acc_male_baseline - acc_female_baseline)*100:.2f}%")

# Confusion matrix
y_pred = np.argmax(baseline_model.predict(X_test), axis=1)
y_true = np.argmax(y_test_one_hot, axis=1)
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=emotion_encoder.classes_,
            yticklabels=emotion_encoder.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

"""# **code in the paper**"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, MaxPooling2D,
                                     GlobalAveragePooling2D, Dense, Dropout, Flatten, Lambda,
                                     LayerNormalization, MultiHeadAttention, Add)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ----------------------------------------------------------------------
# 1ï¸âƒ£ Transformer Encoder Block
# ----------------------------------------------------------------------
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.2):
    x = LayerNormalization(epsilon=1e-6)(inputs)
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = Dropout(dropout)(x)
    res = Add()([x, inputs])

    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation="relu")(x)
    x = Dropout(dropout)(x)
    x = Dense(inputs.shape[-1])(x)
    return Add()([x, res])

# ----------------------------------------------------------------------
# 2ï¸âƒ£ Model Definition
# ----------------------------------------------------------------------
def create_emotion_model(input_shape, num_emotions):
    inputs = Input(shape=input_shape)
    x = Conv2D(16, (5,5), padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling2D((1,2))(x)  # reduces time frames

    x = Conv2D(32, (3,3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2))(x)

    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2))(x)

    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = GlobalAveragePooling2D()(x)
    x = Dense(64, activation='relu')(x)
    x = Lambda(lambda t: tf.expand_dims(t, axis=1), output_shape=(1, 64))(x)

    shared_output = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.3)
    shared_output = Flatten()(shared_output)
    emotion_output = Dense(num_emotions, activation='softmax')(shared_output)
    model = Model(inputs=inputs, outputs=emotion_output)
    return model

# ----------------------------------------------------------------------
# 3ï¸âƒ£ Load and Filter Data
# ----------------------------------------------------------------------
npy_dir = "/content/drive/MyDrive/ears_data/mfcc_segments_23emotions"
selected_emotions = ["adoration", "anger", "fear", "neutral", "sadness"]

segments, labels, speakers = [], [], []
all_files = [f for f in os.listdir(npy_dir) if f.endswith('.npy')]

for f in all_files:
    if "emo_" in f:
        emotion = f.split("emo_")[1].split("_")[0]
        if emotion not in selected_emotions:
            continue
    else:
        continue
    seg_data = np.load(os.path.join(npy_dir, f))
    speaker = f.split("_")[0]
    segments.append(seg_data)
    labels.append(emotion)
    speakers.append(speaker)

X = np.array(segments)[..., np.newaxis]
y_emotions = np.array(labels)
y_speakers = np.array(speakers)

# ----------------------------------------------------------------------
# 4ï¸âƒ£ Define Speaker Splits & Gender Map
# ----------------------------------------------------------------------
train_speakers_male = ['p007','p008','p010','p017','p023','p040','p046','p065','p071','p076','p086','p090','p094','p095','p101']
train_speakers_female = ['p002','p006','p024','p026','p041','p043','p047','p055','p063','p067','p080','p083','p084','p104','p098']
val_speakers_male = ['p019','p025','p038','p039','p102']
val_speakers_female = ['p027','p034','p037','p052','p079']
test_speakers_male = ['p001','p074','p081','p085','p105']
test_speakers_female = ['p012','p036','p061','p068','p073']

# Combine train/val/test speakers
train_speakers = train_speakers_male + train_speakers_female
val_speakers = val_speakers_male + val_speakers_female
test_speakers = test_speakers_male + test_speakers_female

# gender map: 0=male, 1=female
gender_map = {spk:0 for spk in train_speakers_male + val_speakers_male + test_speakers_male}
gender_map.update({spk:1 for spk in train_speakers_female + val_speakers_female + test_speakers_female})
y_gender = np.array([gender_map[spk] for spk in y_speakers if spk in gender_map])

# Remove speakers not in split
valid_mask = np.isin(y_speakers, list(gender_map.keys()))
X, y_emotions, y_speakers = X[valid_mask], y_emotions[valid_mask], y_speakers[valid_mask]
y_gender = np.array([gender_map[spk] for spk in y_speakers])

# Split indices
train_ids = np.array(train_speakers_male + train_speakers_female + val_speakers_male + val_speakers_female + test_speakers_male + test_speakers_female)
train_mask = np.isin(y_speakers, train_speakers_male + train_speakers_female)
val_mask = np.isin(y_speakers, val_speakers_male + val_speakers_female)
test_mask = np.isin(y_speakers, test_speakers_male + test_speakers_female)

X_train, y_train_emo = X[train_mask], y_emotions[train_mask]
X_val, y_val_emo = X[val_mask], y_emotions[val_mask]
X_test, y_test_emo = X[test_mask], y_emotions[test_mask]
y_test_gender = y_gender[test_mask]

# ----------------------------------------------------------------------
# 5ï¸âƒ£ Standardize Data
# ----------------------------------------------------------------------
X_train_3d = np.squeeze(X_train)
X_val_3d   = np.squeeze(X_val)
X_test_3d  = np.squeeze(X_test)

X_train_flat = X_train_3d.reshape(len(X_train_3d), -1)
X_val_flat   = X_val_3d.reshape(len(X_val_3d), -1)
X_test_flat  = X_test_3d.reshape(len(X_test_3d), -1)

scaler = StandardScaler()
scaler.fit(X_train_flat)

X_train_scaled = scaler.transform(X_train_flat).reshape(X_train_3d.shape)[..., np.newaxis]
X_val_scaled   = scaler.transform(X_val_flat).reshape(X_val_3d.shape)[..., np.newaxis]
X_test_scaled  = scaler.transform(X_test_flat).reshape(X_test_3d.shape)[..., np.newaxis]

X_train, X_val, X_test = X_train_scaled, X_val_scaled, X_test_scaled

# Encode emotions
emotion_encoder = LabelEncoder()
emotion_encoder.fit(np.concatenate([y_train_emo, y_val_emo, y_test_emo]))
num_emotions = len(emotion_encoder.classes_)
y_train_one_hot = to_categorical(emotion_encoder.transform(y_train_emo), num_classes=num_emotions)
y_val_one_hot   = to_categorical(emotion_encoder.transform(y_val_emo), num_classes=num_emotions)
y_test_one_hot  = to_categorical(emotion_encoder.transform(y_test_emo), num_classes=num_emotions)

# ----------------------------------------------------------------------
# 6ï¸âƒ£ Train Model with ReduceLROnPlateau
# ----------------------------------------------------------------------
baseline_model = create_emotion_model(X_train.shape[1:], num_emotions)
baseline_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
baseline_model.summary()

reduce_lr = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

history = baseline_model.fit(
    X_train, y_train_one_hot,
    validation_data=(X_val, y_val_one_hot),
    epochs=50,
    batch_size=64,
    verbose=1,
    callbacks=[EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True),
               reduce_lr]
)

# ----------------------------------------------------------------------
# 7ï¸âƒ£ Evaluate Gender-wise Performance
# ----------------------------------------------------------------------
male_mask = y_test_gender == 0
female_mask = y_test_gender == 1
X_test_male, y_test_male = X_test[male_mask], y_test_one_hot[male_mask]
X_test_female, y_test_female = X_test[female_mask], y_test_one_hot[female_mask]

print("\n--- Baseline Model Performance ---")
_, acc_male_baseline = baseline_model.evaluate(X_test_male, y_test_male, verbose=0)
_, acc_female_baseline = baseline_model.evaluate(X_test_female, y_test_female, verbose=0)
print(f"Accuracy on Male Speakers: {acc_male_baseline*100:.2f}%")
print(f"Accuracy on Female Speakers: {acc_female_baseline*100:.2f}%")
print(f"PERFORMANCE GAP: {abs(acc_male_baseline - acc_female_baseline)*100:.2f}%")

# Confusion matrix
y_pred = np.argmax(baseline_model.predict(X_test), axis=1)
y_true = np.argmax(y_test_one_hot, axis=1)
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=emotion_encoder.classes_,
            yticklabels=emotion_encoder.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

x = Conv2D(64, (3,3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2))(x)

    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = GlobalAveragePooling2D()(x)

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, MaxPooling2D,
                                     GlobalAveragePooling2D, Dense, Dropout, Flatten)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import matplotlib.pyplot as plt
import seaborn as sns

# ----------------------------------------------------------------------
# 1ï¸âƒ£ Paths & Parameters
# ----------------------------------------------------------------------
DATA_ROOT = "/content/drive/MyDrive/ears_data/features_mel_image23"
selected_emotions = [
    "adoration", "anger", "fear", "neutral", "sadness"
]
IMG_SIZE = (128, 128)  # Resize mel images to 128x128
CHANNELS = 3           # RGB

# ----------------------------------------------------------------------
# 2ï¸âƒ£ Load Data Speaker-wise
# ----------------------------------------------------------------------
segments, labels, speakers = [], [], []

all_speakers = sorted(os.listdir(DATA_ROOT))

for spk in all_speakers:
    spk_dir = os.path.join(DATA_ROOT, spk)
    if not os.path.isdir(spk_dir):
        continue

    for img_file in os.listdir(spk_dir):
        if img_file.endswith(".png") and any(f"emo_{emo}_" in img_file for emo in selected_emotions):
            emotion = [emo for emo in selected_emotions if f"emo_{emo}_" in img_file][0]

            img_path = os.path.join(spk_dir, img_file)
            img = load_img(img_path, target_size=IMG_SIZE)
            img_array = img_to_array(img) / 255.0  # normalize

            segments.append(img_array)
            labels.append(emotion)
            speakers.append(spk)

X = np.array(segments, dtype=np.float32)
y = np.array(labels)
speakers = np.array(speakers)
print(f"âœ… Loaded {X.shape[0]} images from {len(selected_emotions)} emotions.")

# ----------------------------------------------------------------------
# 3ï¸âƒ£ Speaker-based Split (Same as before)
# ----------------------------------------------------------------------
train_speakers_male = ['p007','p008','p010','p017','p023','p040','p046','p065','p071','p076','p086','p090','p094','p095','p101']
train_speakers_female = ['p002','p006','p024','p026','p041','p043','p047','p055','p063','p067','p080','p083','p084','p104','p098']
val_speakers_male = ['p019','p025','p038','p039','p102']
val_speakers_female = ['p027','p034','p037','p052','p079']
test_speakers_male = ['p001','p074','p081','p085','p105']
test_speakers_female = ['p012','p036','p061','p068','p073']

train_speakers = train_speakers_male + train_speakers_female
val_speakers = val_speakers_male + val_speakers_female
test_speakers = test_speakers_male + test_speakers_female

train_idx = np.isin(speakers, train_speakers)
val_idx = np.isin(speakers, val_speakers)
test_idx = np.isin(speakers, test_speakers)

X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]
y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")

# ----------------------------------------------------------------------
# 4ï¸âƒ£ Label Encoding
# ----------------------------------------------------------------------
emotion_encoder = LabelEncoder()
emotion_encoder.fit(np.concatenate([y_train, y_val, y_test]))
num_emotions = len(emotion_encoder.classes_)

y_train_one_hot = to_categorical(emotion_encoder.transform(y_train), num_classes=num_emotions)
y_val_one_hot = to_categorical(emotion_encoder.transform(y_val), num_classes=num_emotions)
y_test_one_hot = to_categorical(emotion_encoder.transform(y_test), num_classes=num_emotions)

# ----------------------------------------------------------------------
# 5ï¸âƒ£ 2D CNN Model
# ----------------------------------------------------------------------
def create_cnn_model(input_shape, num_classes):
    inputs = Input(shape=input_shape)
    x = Conv2D(32, (3,3), activation='relu', padding='same')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2))(x)
    x = GlobalAveragePooling2D()(x)


    x = Dense(64, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

model = create_cnn_model(X_train.shape[1:], num_emotions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ----------------------------------------------------------------------
# 6ï¸âƒ£ Train
# ----------------------------------------------------------------------
early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, min_lr=1e-6, verbose=1)

history = model.fit(
    X_train, y_train_one_hot,
    validation_data=(X_val, y_val_one_hot),
    batch_size=32,
    epochs=10,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# ----------------------------------------------------------------------
# 7ï¸âƒ£ Separate Testing by Gender
# ----------------------------------------------------------------------
gender_map = {spk:0 for spk in (train_speakers_male + val_speakers_male + test_speakers_male)}
gender_map.update({spk:1 for spk in (train_speakers_female + val_speakers_female + test_speakers_female)})

y_test_gender = np.array([gender_map[spk] for spk in speakers[test_idx]])
y_true = np.argmax(y_test_one_hot, axis=1)
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

male_mask = (y_test_gender == 0)
female_mask = (y_test_gender == 1)

# ---- Male
y_true_male = y_true[male_mask]
y_pred_male = y_pred[male_mask]
male_acc = np.mean(y_pred_male == y_true_male)
male_precision = precision_score(y_true_male, y_pred_male, average='macro', zero_division=0)
male_recall = recall_score(y_true_male, y_pred_male, average='macro', zero_division=0)
male_f1 = f1_score(y_true_male, y_pred_male, average='macro', zero_division=0)

print("\n--- ðŸ‘¨ Male Test Results ---")
print(f"Accuracy : {male_acc*100:.2f}%")
print(f"Precision: {male_precision*100:.2f}%")
print(f"Recall   : {male_recall*100:.2f}%")
print(f"F1-score : {male_f1*100:.2f}%")
print(classification_report(y_true_male, y_pred_male, target_names=emotion_encoder.classes_))
cm_male = confusion_matrix(y_true_male, y_pred_male)
plt.figure(figsize=(7,6))
sns.heatmap(cm_male, annot=True, fmt='d', cmap='Blues',
            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)
plt.title("ðŸ‘¨ Confusion Matrix â€“ Male Test Set")
plt.show()

# ---- Female
y_true_female = y_true[female_mask]
y_pred_female = y_pred[female_mask]
female_acc = np.mean(y_pred_female == y_true_female)
female_precision = precision_score(y_true_female, y_pred_female, average='macro', zero_division=0)
female_recall = recall_score(y_true_female, y_pred_female, average='macro', zero_division=0)
female_f1 = f1_score(y_true_female, y_pred_female, average='macro', zero_division=0)

print("\n--- ðŸ‘© Female Test Results ---")
print(f"Accuracy : {female_acc*100:.2f}%")
print(f"Precision: {female_precision*100:.2f}%")
print(f"Recall   : {female_recall*100:.2f}%")
print(f"F1-score : {female_f1*100:.2f}%")
print(classification_report(y_true_female, y_pred_female, target_names=emotion_encoder.classes_))
cm_female = confusion_matrix(y_true_female, y_pred_female)
plt.figure(figsize=(7,6))
sns.heatmap(cm_female, annot=True, fmt='d', cmap='Reds',
            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)
plt.title("ðŸ‘© Confusion Matrix â€“ Female Test Set")
plt.show()

# ---- Difference
diff_acc = male_acc - female_acc
print("\n--- ðŸ” Performance Difference (Male - Female) ---")
print(f"Accuracy Difference : {diff_acc*100:.2f}%")
print(f"Absolute Accuracy Gap : {abs(diff_acc)*100:.2f}%")

# ---- Overall
overall_acc = np.mean(y_pred == y_true)
overall_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
overall_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
overall_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

print("\n--- âš–ï¸ Overall Combined Results ---")
print(f"Overall Accuracy : {overall_acc*100:.2f}%")
print(f"Overall Precision: {overall_precision*100:.2f}%")
print(f"Overall Recall   : {overall_recall*100:.2f}%")
print(f"Overall F1-score : {overall_f1*100:.2f}%")
cm_overall = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm_overall, annot=True, fmt='d', cmap='Purples',
            xticklabels=emotion_encoder.classes_, yticklabels=emotion_encoder.classes_)
plt.title("âš–ï¸ Confusion Matrix â€“ Combined Male + Female Test Set")
plt.show()